{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bf548da",
   "metadata": {},
   "source": [
    "# PEFT (Parameter-Efficient Fine-Tuning)\n",
    "## 前言：為什麼需要 PEFT？\n",
    "\n",
    "學會了 SFT、DPO、PPO 等對齊方法的「目標函數」和「訓練邏輯」。但當你真正要在自己的機器上跑這些訓練時，會遇到一個殘酷的現實：\n",
    "\n",
    "### 全參數 Fine-tuning 的資源需求\n",
    "\n",
    "**以 LLaMA-2 7B 為例：**\n",
    "\n",
    "- 模型參數量：7B (70億)\n",
    "- 每個參數：2 bytes (fp16) 或 4 bytes (fp32)\n",
    "\n",
    "**只是載入模型：**\n",
    "\n",
    "- 模型權重: 7B × 2 bytes = 14 GB\n",
    "\n",
    "**訓練時需要的記憶體：**\n",
    "\n",
    "- 梯度: 7B × 2 bytes = 14 GB\n",
    "- 優化器狀態 (AdamW): 7B × 8 bytes = 56 GB (m, v 各 4 bytes)\n",
    "- 激活值 (activations): ~20-40 GB (視 batch size)\n",
    "- 總計: 14 + 14 + 56 + 30 = 114 GB\n",
    "\n",
    "**一張 A100 (80GB) 都不夠用**\n",
    "\n",
    "**更大的模型更誇張：**\n",
    "\n",
    "- LLaMA-2 13B: ~200 GB\n",
    "- LLaMA-2 70B: ~1 TB\n",
    "- GPT-3 175B: ~2.5 TB\n",
    "\n",
    "**全參數訓練 70B 模型需要：**\n",
    "\n",
    "- 8-16 張 A100 (80GB)\n",
    "- 分散式訓練框架\n",
    "- 幾萬到幾十萬美金的算力成本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a569265e",
   "metadata": {},
   "source": [
    "### 問題不只是記憶體\n",
    "\n",
    "1. 訓練時間長\n",
    "    - 更新所有參數 → 每步更慢\n",
    "    - 70B 模型一步可能要幾秒\n",
    "2. 過擬合風險高\n",
    "    - 可調參數太多 → 容易記住訓練資料\n",
    "    - 特別是資料量小的時候\n",
    "3. 部署困難\n",
    "    - 每個任務要存一份完整模型\n",
    "    - 10 個任務 = 10 × 14GB = 140GB\n",
    "4. 訓練不穩定\n",
    "    - 參數空間巨大 → 容易發散\n",
    "    - 需要極小的學習率和精細調參"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b18baf8",
   "metadata": {},
   "source": [
    "## PEFT 的核心理念\n",
    "\n",
    "**核心理念：** 凍結絕大部分參數，只訓練極少量的新增參數\n",
    "\n",
    "- Full Fine-tuning：更新 7B 參數\n",
    "- PEFT：更新~10M 參數 (0.14%)\n",
    "- 記憶體需求：114 GB → 20 GB\n",
    "- 訓練速度：1x → 3-5x\n",
    "- 儲存成本：14 GB/任務 → 10 MB/任務\n",
    "- 效果損失：通常 < 5%\n",
    "\n",
    "**LLM 的 fine-tuning 發生在低秩（low-rank）子空間中**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78584d7c",
   "metadata": {},
   "source": [
    "### 基本想法\n",
    "\n",
    "原始模型：W ∈ R^(d×k)  (例如 4096×4096)\n",
    "\n",
    "**Full Fine-tuning**： W_new = W + ΔW\n",
    "\n",
    "- 其中 ΔW 的每個元素都可能改變\n",
    "\n",
    "**PEFT**：W_new = W + low_rank_adaptation\n",
    "\n",
    "- 其中 adaptation 只有很少的自由度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416006b5",
   "metadata": {},
   "source": [
    "### 為什麼low-rank假設有效？\n",
    "假設我們有一個已經訓練好的大模型，它的權重叫做 W_original。現在我們拿這個模型去做全參數 fine-tune，得到一組新的權重 W_finetuned。如果我們把兩者相減，得到的差值：\n",
    "$$ΔW = W_{finetuned} − W_{original}$$\n",
    "\n",
    "過去的實驗發現，這個 ΔW 雖然看起來是一個超大的矩陣（例如 4096×4096），理論上rank可以高達 4096，但實際上並不是這樣。你如果對 ΔW 做 SVD，會發現前面只有非常少數幾個奇異值就已經解釋了幾乎全部的變化量。像是前 16 個奇異值，就能吃掉 95% 以上的能量。\n",
    "\n",
    "用白話講就是：模型在 fine-tune 的時候，真的有在學的新東西，其實集中在一個很低維的方向空間裡。它並沒有用滿整個 4096 維的自由度，而是只在其中一小撮「重要方向」上做調整。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3216bcd",
   "metadata": {},
   "source": [
    "這個觀察就是 LoRA 的核心直覺。既然真正有用的更新只活在一個低維子空間，那我們幹嘛還要傻傻地更新整個 4096×4096 的權重矩陣？參數量爆炸、VRAM又貴，完全沒必要。\n",
    "\n",
    "**LoRA 的做法就是**：原本的權重 W_original 我們凍結不動，然後額外加上一個「低秩更新項」。這個更新項通常被拆成兩個小矩陣相乘，一個是把高維空間壓到低維，另一個再把低維投回原本的維度。這樣一來，我們實際學的參數量就只跟那個低維的 rank 有關，比如 8、16、32，而不是 4096²。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c4a47f",
   "metadata": {},
   "source": [
    "### PEFT 的統一框架\n",
    "**所有 PEFT 方法都遵循相同的模式：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca74096",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PEFTMethod:\n",
    "    def __init__(self, pretrained_model):\n",
    "        self.backbone = pretrained_model\n",
    "        self.backbone.requires_grad_(False)  # 凍結\n",
    "        \n",
    "        self.trainable_params = self.create_efficient_params()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 原始模型的輸出\n",
    "        base_output = self.backbone(x)\n",
    "        \n",
    "        # PEFT 的調整\n",
    "        adaptation = self.apply_efficient_params(x)\n",
    "        \n",
    "        # 組合\n",
    "        return base_output + adaptation\n",
    "    \n",
    "    def create_efficient_params(self):\n",
    "        \"\"\"各種 PEFT 方法的差異在這裡\"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d9c20a",
   "metadata": {},
   "source": [
    "## 主流 PEFT 方法分類\n",
    "### 方法概覽\n",
    "\n",
    "**Additive Methods (加法式)**\n",
    "- Adapter\n",
    "\n",
    "    └─ 在層之間插入小型可訓練模組\n",
    "- Prefix Tuning\n",
    "\n",
    "    └─ 在輸入層添加可訓練的前綴向量\n",
    "- Prompt Tuning\n",
    "\n",
    "    └─ 只在最前面加可訓練的 soft prompt\n",
    "\n",
    "**Selective Methods (選擇式)**\n",
    "- BitFit\n",
    "\n",
    "    └─ 只訓練 bias 參數\n",
    "\n",
    "- Partial Fine-tuning\n",
    "\n",
    "    └─ 只訓練特定層（如最後幾層）\n",
    "\n",
    "**Reparameterization Methods (重參數化)**\n",
    "- LoRA (Low-Rank Adaptation)\n",
    "\n",
    "    └─ 用低秩矩陣分解來近似 ΔW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fd155b",
   "metadata": {},
   "source": [
    "### 方法比較表\n",
    "\n",
    "| 方法 | 可訓練參數量 | 推理延遲 | 記憶體效率 | 效果 | 實作複雜度 |\n",
    "|------|------------|---------|-----------|------|-----------|\n",
    "| **LoRA** | 0.1-1% | 無 | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | 低 |\n",
    "| **Adapter** | 0.5-2% | 有 | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | 中 |\n",
    "| **Prefix Tuning** | 0.01-0.1% | 有 | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | 中 |\n",
    "| **Prompt Tuning** | <0.01% | 有 | ⭐⭐⭐⭐⭐ | ⭐⭐ | 低 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73634d17",
   "metadata": {},
   "source": [
    "## LoRA：為什麼成為主流？\n",
    "### 核心想法\n",
    "\n",
    "**LoRA (Low-Rank Adaptation) 的想法極其簡單：**\n",
    "\n",
    "**原始權重矩陣**： W ∈ R^(d×k)\n",
    "\n",
    "**Full Fine-tuning**：W' = W + ΔW  (ΔW 是 d×k 的滿秩矩陣)\n",
    "\n",
    "**LoRA**：W' = W + BA\n",
    "\n",
    "其中：\n",
    "- B ∈ R^(d×r)\n",
    "- A ∈ R^(r×k)\n",
    "- r << min(d, k)  (rank，通常 r=8,16,32)\n",
    "\n",
    "**參數量比較：**\n",
    "d=4096, k=4096, r=16\n",
    "\n",
    "**Full**: d × k = 16,777,216 參數\n",
    "\n",
    "**LoRA**: d × r + r × k = r(d + k) = 131,072 參數 (0.78%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818f1ebd",
   "metadata": {},
   "source": [
    "### 為什麼 LoRA 這麼受歡迎？\n",
    "\n",
    "**優勢 1：零推理延遲**\n",
    "\n",
    "LoRA 在訓練時會多學一組小矩陣，但在部署前可以直接把這些權重合併回原本的模型，只需要做一次合併就好。合併之後，推理時的計算流程和原本模型完全一樣，不會多出任何額外運算，因此幾乎沒有推理延遲。這一點跟 Adapter 很不一樣，Adapter 在推理時還得多走一次前向傳播，自然就會拖慢速度。\n",
    "\n",
    "**優勢 2：模組化切換**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf758b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多任務部署\n",
    "base_model = load_pretrained()\n",
    "\n",
    "# 載入不同任務的 LoRA 權重\n",
    "lora_A_task1, lora_B_task1 = load_lora(\"task1\")\n",
    "lora_A_task2, lora_B_task2 = load_lora(\"task2\")\n",
    "\n",
    "# 快速切換\n",
    "def switch_task(task_id):\n",
    "    if task_id == 1:\n",
    "        apply_lora(lora_A_task1, lora_B_task1)\n",
    "    elif task_id == 2:\n",
    "        apply_lora(lora_A_task2, lora_B_task2)\n",
    "\n",
    "\n",
    "# 儲存空間\n",
    "\"\"\"\n",
    "Base model: 14 GB\n",
    "LoRA per task: 10-50 MB\n",
    "10 個任務: 14 GB + 10 × 50 MB = 14.5 GB\n",
    "vs Full FT: 10 × 14 GB = 140 GB\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e704082d",
   "metadata": {},
   "source": [
    "**優勢 3：訓練穩定**\n",
    "\n",
    "LoRA 的初始化設計讓訓練一開始不會影響原本的預訓練權重，模型等於是從原狀態起跑，再慢慢學會怎麼調整。這樣可以避免訓練初期的不穩定，讓模型用更平滑、漸進的方式進行微調。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c72b769",
   "metadata": {},
   "source": [
    "## LoRA 的超參數\n",
    "\n",
    "### r (Rank)：核心超參數\n",
    "\n",
    "**經驗法則：**\n",
    "- 任務複雜度低（如分類）：r = 4, 8\n",
    "\n",
    "- 一般對齊任務（SFT / DPO）：r = 16, 32\n",
    "\n",
    "- 複雜領域適應：r = 64, 128\n",
    "\n",
    "r 的選擇本質上是在模型表達能力與資源成本之間取捨。較大的 r 能提供更強的參數自由度，通常有助於提升任務效果，但同時會增加可訓練參數數量、記憶體佔用與訓練成本，並提高過擬合的風險；相反地，較小的 r 雖然效率高、穩定性好，但可能限制模型的學習能力。\n",
    "\n",
    "實務上可先以 r=16 作為基準設定，快速建立 baseline。若效果不足，再逐步提升至 r=32 或 64 觀察增益；若發現驗證集表現下降或出現過擬合跡象，則可反向嘗試較小的 r（如 8 或 4），以提高泛化能力並降低資源消耗。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de26c685",
   "metadata": {},
   "source": [
    "### α (LoRA Alpha)：縮放係數\n",
    "\n",
    "LoRA 在前向傳播時，會在原始權重輸出之外加入一個低秩更新項，其形式為 output = W @ x + (α / r) × B @ A @ x。\n",
    "\n",
    "其中 (α / r) 是用來控制 LoRA 更新對整體輸出的影響比例。\n",
    "\n",
    "**α 的作用**：α（alpha）負責調整 LoRA 更新的整體強度，可視為低秩權重的全域縮放係數。即使 r 固定，透過改變 α，也能讓模型對新任務的適應程度變得更激進或更保守。\n",
    "\n",
    "**常見設定：**\n",
    "- α = r  → 縮放因子 = 1（標準設定）\n",
    "\n",
    "- α = 2r  → 縮放因子 = 2（較激進）\n",
    "\n",
    "- α = r/2 → 縮放因子 = 0.5（較保守）\n",
    "\n",
    "在大多數任務中，α=r 是穩定且效果可靠的預設選擇。若微調的是已對齊良好的模型、且希望模型更快速偏向新任務，可考慮提高 α；反之，若希望盡量保留原模型行為、僅做小幅調整，則降低 α 有助於控制變化幅度。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70afc74",
   "metadata": {},
   "source": [
    "### Target Modules：應用 LoRA 的位置\n",
    "\n",
    "Target Modules 指的是在模型中實際套用 LoRA 的權重矩陣位置。由於 LoRA 並不需要對所有參數進行微調，因此透過選擇性地指定 target modules，可以在「效能、參數量與訓練成本」之間取得平衡。不同模組負責的功能不同，對最終行為的影響程度也有所差異，這使得 target modules 的選擇成為實務中非常重要的設計決策。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd550d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 的權重矩陣\n",
    "attention_weights = {\n",
    "    'q_proj': Query 投影,\n",
    "    'k_proj': Key 投影,\n",
    "    'v_proj': Value 投影,\n",
    "    'o_proj': Output 投影,\n",
    "}\n",
    "\n",
    "ffn_weights = {\n",
    "    'gate_proj': FFN 第一層,\n",
    "    'up_proj': FFN 第二層,\n",
    "    'down_proj': FFN 輸出層,\n",
    "}\n",
    "\n",
    "# 常見配置\n",
    "\n",
    "# 配置 1: 只訓練 Q, V (最省資源)\n",
    "target_modules = ['q_proj', 'v_proj']\n",
    "# 參數量: 最少\n",
    "# 效果: 中等\n",
    "# 適用: 資源極度受限\n",
    "\n",
    "# 配置 2: 訓練所有 attention (標準)\n",
    "target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
    "# 參數量: 中\n",
    "# 效果: 好\n",
    "# 適用: 大多數場景\n",
    "\n",
    "# 配置 3: Attention + FFN (最全面)\n",
    "target_modules = [\n",
    "    'q_proj', 'k_proj', 'v_proj', 'o_proj',\n",
    "    'gate_proj', 'up_proj', 'down_proj'\n",
    "]\n",
    "# 參數量: 最多\n",
    "# 效果: 最好\n",
    "# 適用: 需要強領域適應"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346fd622",
   "metadata": {},
   "source": [
    "## LoRA 實作範例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3f3e3a",
   "metadata": {},
   "source": [
    "### 載入基礎模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e622c515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c7e127",
   "metadata": {},
   "source": [
    "### 配置 LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d02c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    \n",
    "    # LLaMA 常見設定\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",                        # bias 訓練策略\n",
    "    task_type=TaskType.CAUSAL_LM,       # 任務類型\n",
    "    inference_mode=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339cce0f",
   "metadata": {},
   "source": [
    "### 訓練 LoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531432aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(base_model, lora_config)\n",
    "model.train()\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_output\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,                 # LoRA 可用較大學習率\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    # data_collator=data_collator,  # Causal LM 常見\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model.save_pretrained(\"./lora_weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8785fbab",
   "metadata": {},
   "source": [
    "### 載入 LoRA 進行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97259db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, \"./lora_weights\")\n",
    "model.eval()\n",
    "\n",
    "# 推理\n",
    "output = model.generate(...)\n",
    "\n",
    "\n",
    "# (可選) 合併權重以加速推理\n",
    "#model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c0bda1",
   "metadata": {},
   "source": [
    "## 其他 PEFT 方法簡介\n",
    "### Adapter\n",
    "\n",
    "**核心想法：** 在 Transformer 的每一層插入小型可訓練的瓶頸模組\n",
    "\n",
    "![IMG](https://truth.bahamut.com.tw/s01/202602/47de79fd93b79f888bb304db164dfee7.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883408df",
   "metadata": {},
   "source": [
    "**Adapter 的結構：**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a083b7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adapter(nn.Module):\n",
    "    def __init__(self, d_model, bottleneck_dim):\n",
    "        super().__init__()\n",
    "        # 降維\n",
    "        self.down_proj = nn.Linear(d_model, bottleneck_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        # 升維\n",
    "        self.up_proj = nn.Linear(bottleneck_dim, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 瓶頸結構\n",
    "        h = self.down_proj(x)       # d_model → bottleneck\n",
    "        h = self.activation(h)\n",
    "        h = self.up_proj(h)         # bottleneck → d_model\n",
    "        return x + h                # 殘差連接"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34b16d8",
   "metadata": {},
   "source": [
    "**參數量**\n",
    "- d_model = 4096\n",
    "\n",
    "- bottleneck = 256\n",
    "\n",
    "- 參數量 = 4096×256 + 256×4096 = 2,097,152  (每個 adapter)\n",
    "\n",
    "**優缺點：**\n",
    "\n",
    "- 簡單直觀\n",
    "- 靈活（可調整瓶頸大小）\n",
    "- 推理時有額外計算（不像 LoRA 可合併）\n",
    "- 參數量比 LoRA 略多"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb13a58",
   "metadata": {},
   "source": [
    "### Prefix Tuning\n",
    "\n",
    "**核心想法：** 在每層的 Key 和 Value 前面添加可訓練的前綴向量\n",
    "\n",
    "\n",
    "**原始 Attention：**\n",
    "- Q = Input × W_q\n",
    "\n",
    "- K = Input × W_k\n",
    "\n",
    "- V = Input × W_v\n",
    "\n",
    "- Attention(Q, K, V)\n",
    "\n",
    "**Prefix Tuning：**\n",
    "\n",
    "- Q = Input × W_q\n",
    "\n",
    "- K = [Prefix_K; Input × W_k]  ← 拼接可訓練前綴\n",
    "\n",
    "- V = [Prefix_V; Input × W_v]  ← 拼接可訓練前綴\n",
    "\n",
    "- Attention(Q, K, V)\n",
    "\n",
    "**實作：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b7ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrefixTuning(nn.Module):\n",
    "    def __init__(self, num_layers, num_heads, d_head, prefix_len):\n",
    "        super().__init__()\n",
    "        # 每層都有自己的 prefix\n",
    "        self.prefix_k = nn.Parameter(\n",
    "            torch.randn(num_layers, prefix_len, num_heads, d_head)\n",
    "        )\n",
    "        self.prefix_v = nn.Parameter(\n",
    "            torch.randn(num_layers, prefix_len, num_heads, d_head)\n",
    "        )\n",
    "    \n",
    "    def forward(self, layer_idx, k, v):\n",
    "        # k, v: [batch, seq_len, num_heads, d_head]\n",
    "        prefix_k = self.prefix_k[layer_idx].unsqueeze(0)  # [1, prefix_len, ...]\n",
    "        prefix_v = self.prefix_v[layer_idx].unsqueeze(0)\n",
    "        \n",
    "        # 拼接\n",
    "        k = torch.cat([prefix_k.expand(k.size(0), -1, -1, -1), k], dim=1)\n",
    "        v = torch.cat([prefix_v.expand(v.size(0), -1, -1, -1), v], dim=1)\n",
    "        \n",
    "        return k, v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399f5f29",
   "metadata": {},
   "source": [
    "**優缺點：**\n",
    "- 參數量極少（< 0.1%）\n",
    "- 不改變模型結構\n",
    "- 推理時佔用序列長度\n",
    "- 效果通常不如 LoRA\n",
    "- 訓練初期不穩定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8862086c",
   "metadata": {},
   "source": [
    "### Prompt Tuning\n",
    "\n",
    "**核心想法：** 只在輸入層添加可訓練的 soft prompts\n",
    "\n",
    "**原始輸入：**[User instruction tokens]\n",
    "\n",
    "**Prompt Tuning：**[Learnable soft prompt tokens] + [User instruction tokens]\n",
    "\n",
    "**例子：**\n",
    "- 原始: \"Translate to French: Hello\"\n",
    "- PT:   [v1][v2][v3]...[v10] + \"Translate to French: Hello\"\n",
    "\n",
    "其中 v1...v10 是可學習的向量，不對應任何真實 token\n",
    "\n",
    "**實作：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655aa710",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptTuning(nn.Module):\n",
    "    def __init__(self, num_prompts, embedding_dim):\n",
    "        super().__init__()\n",
    "        # 可訓練的 soft prompt embeddings\n",
    "        self.soft_prompt = nn.Parameter(\n",
    "            torch.randn(num_prompts, embedding_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_embeds):\n",
    "        # input_embeds: [batch, seq_len, embedding_dim]\n",
    "        batch_size = input_embeds.size(0)\n",
    "        \n",
    "        # 擴展 soft prompt 到 batch\n",
    "        prompts = self.soft_prompt.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        \n",
    "        # 拼接到輸入前面\n",
    "        return torch.cat([prompts, input_embeds], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2c76ce",
   "metadata": {},
   "source": [
    "**優缺點：**\n",
    "\n",
    "- 參數量最少（幾 KB）\n",
    "\n",
    "- 實作最簡單\n",
    "\n",
    "- 效果最差（尤其是小模型）\n",
    "\n",
    "- 需要大模型（>10B）才有效\n",
    "\n",
    "- 只適合特定類型任務"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87acb46",
   "metadata": {},
   "source": [
    "## LoRA 超參數推薦表\n",
    "\n",
    "### 根據任務類型\n",
    "\n",
    "任務類型          | r    | α    | target_modules           | dropout\n",
    "----------------|------|------|-------------------------|--------\n",
    "分類/標註         | 8    | 16   | q_proj, v_proj          | 0.05\n",
    "QA/抽取式         | 16   | 32   | q,k,v,o_proj            | 0.05\n",
    "對話/指令遵循     | 16   | 32   | q,k,v,o_proj            | 0.05\n",
    "摘要              | 32   | 64   | q,k,v,o + FFN           | 0.1\n",
    "程式碼生成        | 64   | 128  | 全部 attention + FFN     | 0.1\n",
    "數學推理          | 64   | 128  | 全部 attention + FFN     | 0.05\n",
    "多語言            | 128  | 256  | 全部 attention + FFN     | 0.1\n",
    "領域適應 (醫學等) | 64   | 128  | 全部 attention + FFN     | 0.1\n",
    "\n",
    "\n",
    "### 根據模型大小\n",
    "\n",
    "模型大小      | r (起始) | 可嘗試範圍\n",
    "-----------|---------|-------------\n",
    "< 1B       | 4-8     | 4-16\n",
    "1B-7B      | 16      | 8-64\n",
    "7B-13B     | 16-32   | 16-128\n",
    "13B-70B    | 32      | 16-128\n",
    "&gt; 70B      | 32-64   | 32-256\n",
    "\n",
    "\n",
    "### 根據資料量\n",
    "\n",
    "\n",
    "訓練資料量   | r    | 建議\n",
    "----------|------|----------------------------------\n",
    "< 1K      | 4-8  | 極小 r，避免過擬合\n",
    "1K-10K    | 8-16 | 標準設定\n",
    "10K-100K  | 16-32| 可適度增大\n",
    "&gt; 100K    | 32-64| 可用較大 r，類似 Full FT\n",
    "\n",
    "\n",
    "### 學習率設定\n",
    "\n",
    "LoRA 可以用比 Full FT 更大的學習率\n",
    "\n",
    "訓練方法        | 學習率範圍        | 推薦起始值\n",
    "-------------|-----------------|------------\n",
    "Full FT      | 5e-6 ~ 5e-5     | 2e-5\n",
    "LoRA (r≤16)  | 1e-4 ~ 5e-4     | 2e-4\n",
    "LoRA (r>32)  | 5e-5 ~ 2e-4     | 1e-4\n",
    "Adapter      | 1e-4 ~ 1e-3     | 5e-4\n",
    "Prefix Tuning| 5e-5 ~ 5e-4     | 1e-4\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

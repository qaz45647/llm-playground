{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98660640",
   "metadata": {},
   "source": [
    "# SFT 監督式微調"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf006826",
   "metadata": {},
   "source": [
    "## 什麼是 SFT (Supervised Fine-Tuning)?\n",
    "\n",
    "Supervised Fine-Tuning (SFT) 是最直接的微調方法,使用標註好的「輸入-輸出」對來訓練模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21865428",
   "metadata": {},
   "source": [
    "## SFT 的優勢與限制\n",
    "**優勢:**\n",
    "- 概念簡單,易於理解和實現\n",
    "- 資料需求相對明確\n",
    "- 訓練穩定,收斂快\n",
    "- 可以快速適應特定任務\n",
    "**限制:**\n",
    "- 需要大量高質量標註資料\n",
    "- 可能過擬合訓練資料\n",
    "- 對資料質量極為敏感"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c445800e",
   "metadata": {},
   "source": [
    "## 資料格式設計\n",
    "\n",
    "Alpaca 格式（單輪對話）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aae467e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "  \"instruction\": \"將以下英文翻譯成中文\",\n",
    "  \"input\": \"The weather is beautiful today.\",\n",
    "  \"output\": \"今天天氣很好。\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f92dcf",
   "metadata": {},
   "source": [
    "Chat 格式（多輪對話）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fbd795",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "  \"conversations\": [\n",
    "    {\"role\": \"user\", \"content\": \"什麼是機器學習？\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"機器學習是人工智慧的一個分支...\"},\n",
    "    {\"role\": \"user\", \"content\": \"能舉個例子嗎？\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"當然！例如垃圾郵件過濾...\"}\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7e1731",
   "metadata": {},
   "source": [
    "實際使用時的 Prompt 模板："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5c7e29",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "<|system|>你是一個有幫助的 AI 助手</s>\n",
    "<|user|>什麼是機器學習？</s>\n",
    "<|assistant|>機器學習是人工智慧的一個分支...</s>\n",
    "<|user|>能舉個例子嗎？</s>\n",
    "<|assistant|>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02586bf4",
   "metadata": {},
   "source": [
    "模型學習在 <|assistant|> 標記後生成回應。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88367ac",
   "metadata": {},
   "source": [
    "## 資料清洗與去重\n",
    "\n",
    "高品質資料是 SFT 成功的關鍵。常見清洗步驟："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5f989d",
   "metadata": {},
   "source": [
    "### 去除低品質樣本：\n",
    "- 太短（< 10 字）或太長（> 2048 tokens）\n",
    "- 包含亂碼、重複字符（!!!!!!）\n",
    "- 有害、歧視性內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9edf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import Dict\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"清理單個文本\"\"\"\n",
    "    # 移除多餘空白\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # 移除重複標點符號 (!!!!! -> !)\n",
    "    text = re.sub(r'([!?。])\\1+', r'\\1', text)\n",
    "    # 移除特殊控制字符\n",
    "    text = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def is_valid_sample(sample: Dict) -> bool:\n",
    "    instruction = sample.get('instruction', '')\n",
    "    output = sample.get('output', '')\n",
    "\n",
    "    # 1. 長度限制\n",
    "    if len(instruction) < 10 or len(instruction) > 2048:\n",
    "        return False\n",
    "    if len(output) < 10 or len(output) > 2048:\n",
    "        return False\n",
    "\n",
    "    # 2. 亂碼檢查\n",
    "    valid_chars = re.findall(r'[a-zA-Z\\u4e00-\\u9fff]', output)\n",
    "    if len(output) >= 20 and len(valid_chars) < len(output) * 0.5:\n",
    "        return False\n",
    "\n",
    "    # 3. 重複字符\n",
    "    if re.search(r'(.)\\1{8,}', output):\n",
    "        return False\n",
    "\n",
    "    # 4. 敏感詞\n",
    "    blacklist = ['炸彈', '黑鬼', '毒品']\n",
    "    text_to_check = instruction + output\n",
    "    if any(word in text_to_check for word in blacklist):\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def clean_dataset(input_file: str, output_file: str):\n",
    "    \"\"\"清洗整個資料集\"\"\"\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    cleaned_data = []\n",
    "    stats = {'total': len(data), 'removed': 0}\n",
    "    \n",
    "    for sample in data:\n",
    "        # 清理文本\n",
    "        sample['instruction'] = clean_text(sample['instruction'])\n",
    "        sample['output'] = clean_text(sample['output'])\n",
    "        if 'input' in sample:\n",
    "            sample['input'] = clean_text(sample['input'])\n",
    "        \n",
    "        # 檢查有效性\n",
    "        if is_valid_sample(sample):\n",
    "            cleaned_data.append(sample)\n",
    "        else:\n",
    "            stats['removed'] += 1\n",
    "    \n",
    "    # 保存清洗後的資料\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(cleaned_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"清洗完成: 原始 {stats['total']} 筆, \"\n",
    "          f\"移除 {stats['removed']} 筆, \"\n",
    "          f\"保留 {len(cleaned_data)} 筆\")\n",
    "\n",
    "# 使用範例\n",
    "clean_dataset('raw_data.json', 'cleaned_data.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da7cb9c",
   "metadata": {},
   "source": [
    "### 去重：\n",
    "**精確去重：移除完全重複的樣本**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c66d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import hashlib\n",
    "from typing import List, Dict\n",
    "\n",
    "def exact_dedup(data: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"精確去重 - 移除完全相同的樣本\"\"\"\n",
    "    seen_hashes = set()\n",
    "    deduped_data = []\n",
    "    duplicate_count = 0\n",
    "\n",
    "    for sample in data:\n",
    "        # 使用分隔符避免字串拼接碰撞\n",
    "        content = sample['instruction'] + \"|||\" + sample['output']\n",
    "        content_hash = hashlib.md5(content.encode('utf-8')).hexdigest()\n",
    "\n",
    "        if content_hash not in seen_hashes:\n",
    "            seen_hashes.add(content_hash)\n",
    "            deduped_data.append(sample)\n",
    "        else:\n",
    "            duplicate_count += 1\n",
    "\n",
    "    print(f\"精確去重: 移除 {duplicate_count} 筆重複樣本\")\n",
    "    return deduped_data\n",
    "\n",
    "\n",
    "# 使用範例\n",
    "with open('cleaned_data.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "deduped_data = exact_dedup(data)\n",
    "\n",
    "with open('deduped_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(deduped_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c039f020",
   "metadata": {},
   "source": [
    "**近似去重（相似的樣本）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf799e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def fuzzy_dedup(data: List[Dict], threshold: float = 0.85) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    近似去重 - 移除高度相似的樣本\n",
    "    threshold: 相似度閾值\n",
    "    \"\"\"\n",
    "    texts = [sample['output'] for sample in data]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "    keep_indices = set(range(len(data)))\n",
    "    batch_size = 1000\n",
    "\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        end_i = min(i + batch_size, len(data))\n",
    "        similarities = cosine_similarity(\n",
    "            tfidf_matrix[i:end_i],\n",
    "            tfidf_matrix\n",
    "        )\n",
    "\n",
    "        for j in range(similarities.shape[0]):\n",
    "            actual_j = i + j\n",
    "            if actual_j not in keep_indices:\n",
    "                continue\n",
    "\n",
    "            # 只考慮「比自己後面」且相似度超過 threshold 的樣本\n",
    "            similar_indices = np.where(\n",
    "                (similarities[j] > threshold) &\n",
    "                (np.arange(len(data)) > actual_j)\n",
    "            )[0]\n",
    "\n",
    "            for k in similar_indices:\n",
    "                if k in keep_indices:\n",
    "                    keep_indices.remove(k)\n",
    "\n",
    "    deduped_data = [data[i] for i in sorted(keep_indices)]\n",
    "    removed = len(data) - len(deduped_data)\n",
    "    print(f\"近似去重: 移除 {removed} 筆相似樣本 (閾值={threshold})\")\n",
    "\n",
    "    return deduped_data\n",
    "\n",
    "\n",
    "# 使用範例\n",
    "with open('deduped_data.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "fuzzy_deduped_data = fuzzy_dedup(data, threshold=0.85)\n",
    "\n",
    "with open('final_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(fuzzy_deduped_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0abed52",
   "metadata": {},
   "source": [
    "**MinHash 快速去重（適合大規模資料）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4266e9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "def minhash_dedup(data: List[Dict], threshold: float = 0.8) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    使用 MinHash LSH 進行快速近似去重\n",
    "    適合大規模資料（10 萬+）\n",
    "    \"\"\"\n",
    "    lsh = MinHashLSH(threshold=threshold, num_perm=128)\n",
    "    minhashes = {}\n",
    "\n",
    "    # 第一遍：建立 MinHash 並插入 LSH\n",
    "    for idx, sample in enumerate(data):\n",
    "        m = MinHash(num_perm=128)\n",
    "        text = sample['output']\n",
    "\n",
    "        # character-level 3-gram\n",
    "        for i in range(len(text) - 2):\n",
    "            m.update(text[i:i+3].encode('utf-8'))\n",
    "\n",
    "        minhashes[idx] = m\n",
    "        lsh.insert(idx, m)\n",
    "\n",
    "    # 第二遍：去重（保留最早出現的）\n",
    "    seen = set()\n",
    "    deduped_data = []\n",
    "\n",
    "    for idx, sample in enumerate(data):\n",
    "        if idx in seen:\n",
    "            continue\n",
    "\n",
    "        similar_indices = lsh.query(minhashes[idx])\n",
    "\n",
    "        # 只標記「比自己 index 大」的相似樣本\n",
    "        for other_idx in similar_indices:\n",
    "            if other_idx > idx:\n",
    "                seen.add(other_idx)\n",
    "\n",
    "        deduped_data.append(sample)\n",
    "\n",
    "    removed = len(data) - len(deduped_data)\n",
    "    print(f\"MinHash 去重: 移除 {removed} 筆相似樣本 (閾值={threshold})\")\n",
    "\n",
    "    return deduped_data\n",
    "\n",
    "\n",
    "# 使用範例\n",
    "# minhash_deduped_data = minhash_dedup(data, threshold=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6af030e",
   "metadata": {},
   "source": [
    "### 平衡資料分佈：\n",
    "- 避免某類任務（如翻譯）佔比過高\n",
    "- 確保不同難度、風格的樣本都有涵蓋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2531fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Dict, Any\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def balance_dataset(\n",
    "    data: List[Dict[str, Any]],\n",
    "    category_key: str = 'category',\n",
    "    max_per_category: int = 1000,\n",
    "    seed: int | None = None\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    平衡不同類別的資料數量\n",
    "    避免某類任務樣本過多導致模型偏向\n",
    "    \"\"\"\n",
    "\n",
    "    if max_per_category <= 0:\n",
    "        raise ValueError(\"max_per_category 必須大於 0\")\n",
    "\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    # 按類別分組\n",
    "    categorized = defaultdict(list)\n",
    "    for sample in data:\n",
    "        category = sample.get(category_key, 'unknown')\n",
    "        categorized[category].append(sample)\n",
    "\n",
    "    # 顯示原始分佈\n",
    "    print(\"原始分佈:\")\n",
    "    for cat, samples in categorized.items():\n",
    "        print(f\"  {cat}: {len(samples)} 筆\")\n",
    "\n",
    "    # 對每個類別進行採樣\n",
    "    balanced_data = []\n",
    "    for category, samples in categorized.items():\n",
    "        if len(samples) > max_per_category:\n",
    "            sampled = random.sample(samples, max_per_category)\n",
    "            print(f\"  {category}: 從 {len(samples)} 降採樣至 {max_per_category}\")\n",
    "        else:\n",
    "            sampled = samples\n",
    "\n",
    "            if len(samples) < max_per_category * 0.1:\n",
    "                print(f\"  ⚠️ {category}: 樣本數偏少 ({len(samples)})\")\n",
    "\n",
    "        balanced_data.extend(sampled)\n",
    "\n",
    "    # 打亂順序\n",
    "    random.shuffle(balanced_data)\n",
    "\n",
    "    print(f\"\\n平衡後總計: {len(balanced_data)} 筆\")\n",
    "    return balanced_data\n",
    "\n",
    "\n",
    "# 使用範例\n",
    "# 假設資料中有 'category' 欄位標記任務類型\n",
    "balanced_data = balance_dataset(\n",
    "    fuzzy_deduped_data,\n",
    "    max_per_category=1000,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36257ee1",
   "metadata": {},
   "source": [
    "### 完整清洗流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3440a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_cleaning_pipeline(input_file: str, output_file: str):\n",
    "    \"\"\"完整的資料清洗流程\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"開始資料清洗流程\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 步驟 1: 載入資料\n",
    "    print(\"\\n[1/5] 載入資料...\")\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"載入 {len(data)} 筆原始資料\")\n",
    "    \n",
    "    # 步驟 2: 基礎清洗\n",
    "    print(\"\\n[2/5] 基礎清洗...\")\n",
    "    cleaned = []\n",
    "    for sample in data:\n",
    "        sample['instruction'] = clean_text(sample['instruction'])\n",
    "        sample['output'] = clean_text(sample['output'])\n",
    "        if is_valid_sample(sample):\n",
    "            cleaned.append(sample)\n",
    "    print(f\"保留 {len(cleaned)} 筆有效資料\")\n",
    "    \n",
    "    # 步驟 3: 精確去重\n",
    "    print(\"\\n[3/5] 精確去重...\")\n",
    "    deduped = exact_dedup(cleaned)\n",
    "    \n",
    "    # 步驟 4: 近似去重\n",
    "    print(\"\\n[4/5] 近似去重...\")\n",
    "    final = fuzzy_dedup(deduped, threshold=0.85)\n",
    "    \n",
    "    # 步驟 5: 保存結果\n",
    "    print(\"\\n[5/5] 保存清洗後資料...\")\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"清洗完成！\")\n",
    "    print(f\"原始資料: {len(data)} 筆\")\n",
    "    print(f\"最終資料: {len(final)} 筆\")\n",
    "    print(f\"移除比例: {(1 - len(final)/len(data))*100:.1f}%\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# 執行完整流程\n",
    "full_cleaning_pipeline('raw_data.json', 'cleaned_final.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a353237",
   "metadata": {},
   "source": [
    "## 資料擴增\n",
    "資料擴增是解決訓練資料不足的關鍵技術。透過自動化方法從有限的資料中生成更多高品質的訓練樣本。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8bc365",
   "metadata": {},
   "source": [
    "### 文字改寫 (Paraphrasing)\n",
    "將原始問題或回答用不同的表達方式重新呈現，保持語義不變但增加表達多樣性。\n",
    "\n",
    "**使用 LLM 進行改寫：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c8a570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "class ParaphraseAugmenter:\n",
    "    def __init__(self, api_key):\n",
    "        self.client = anthropic.Anthropic(api_key=api_key)\n",
    "\n",
    "    def paraphrase_question(self, question, num_variants=3):\n",
    "        \"\"\"生成問題的多種改寫版本\"\"\"\n",
    "\n",
    "        prompt = f\"\"\"請將以下問題改寫成 {num_variants} 種不同的表達方式，保持原意不變。\n",
    "\n",
    "原始問題：\n",
    "{question}\n",
    "\n",
    "要求：\n",
    "1. 每個改寫版本在語氣、用詞或句式上有所不同\n",
    "2. 保持問題的核心意圖和資訊需求\n",
    "3. 語言自然流暢\n",
    "4. 每行輸出一個改寫版本\n",
    "\n",
    "改寫版本：\n",
    "\"\"\"\n",
    "\n",
    "        response = self.client.messages.create(\n",
    "            model=\"claude-sonnet-4-20250514\",\n",
    "            max_tokens=1000,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "\n",
    "        lines = response.content[0].text.strip().split('\\n')\n",
    "\n",
    "        variants = []\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            line = line.lstrip(\"0123456789.-、 \")\n",
    "            variants.append(line)\n",
    "\n",
    "        return variants\n",
    "\n",
    "    def paraphrase_answer(self, answer, style=\"formal\"):\n",
    "        \"\"\"改寫回答，可指定風格\"\"\"\n",
    "\n",
    "        style_prompts = {\n",
    "            \"formal\": \"正式、專業的風格\",\n",
    "            \"casual\": \"輕鬆、口語化的風格\",\n",
    "            \"detailed\": \"更詳細、有更多解釋的風格\",\n",
    "            \"concise\": \"更簡潔、直接的風格\"\n",
    "        }\n",
    "\n",
    "        style_desc = style_prompts.get(style, style_prompts[\"formal\"])\n",
    "\n",
    "        prompt = f\"\"\"請將以下回答改寫成 {style_desc}。\n",
    "\n",
    "原始回答：\n",
    "{answer}\n",
    "\n",
    "改寫回答：\n",
    "\"\"\"\n",
    "\n",
    "        response = self.client.messages.create(\n",
    "            model=\"claude-sonnet-4-20250514\",\n",
    "            max_tokens=2000,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "\n",
    "        return response.content[0].text.strip()\n",
    "\n",
    "\n",
    "# ===== 使用範例 =====\n",
    "\n",
    "augmenter = ParaphraseAugmenter(api_key=\"your_api_key\")\n",
    "\n",
    "question = \"如何在 Python 中讀取 CSV 檔案？\"\n",
    "variants = augmenter.paraphrase_question(question, num_variants=5)\n",
    "\n",
    "print(\"原始問題：\", question)\n",
    "print(\"改寫版本：\")\n",
    "for i, v in enumerate(variants, 1):\n",
    "    print(f\"{i}. {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeb06c0",
   "metadata": {},
   "source": [
    "**使用同義詞替換：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e03918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import jieba\n",
    "\n",
    "class SynonymAugmenter:\n",
    "    def __init__(self):\n",
    "        # 中文同義詞字典\n",
    "        self.synonym_dict = {\n",
    "            \"如何\": [\"怎麼\", \"怎樣\", \"要如何\", \"該如何\"],\n",
    "            \"方法\": [\"方式\", \"做法\", \"途徑\", \"手段\"],\n",
    "            \"使用\": [\"用\", \"運用\", \"利用\", \"採用\"],\n",
    "            \"建立\": [\"創建\", \"建構\", \"設立\", \"製作\"],\n",
    "            \"問題\": [\"疑問\", \"困難\", \"難題\", \"課題\"],\n",
    "            \"解決\": [\"處理\", \"克服\", \"搞定\", \"應對\"],\n",
    "        }\n",
    "\n",
    "    def augment_with_synonyms(self, text, replace_ratio=0.3):\n",
    "        \"\"\"使用同義詞替換部分詞彙\"\"\"\n",
    "\n",
    "        words = list(jieba.cut(text))\n",
    "        if not words:\n",
    "            return text\n",
    "\n",
    "        num_replacements = max(1, int(len(words) * replace_ratio))\n",
    "        replace_indices = random.sample(\n",
    "            range(len(words)),\n",
    "            min(num_replacements, len(words))\n",
    "        )\n",
    "\n",
    "        augmented = words.copy()\n",
    "        for idx in replace_indices:\n",
    "            word = words[idx]\n",
    "            if word in self.synonym_dict:\n",
    "                augmented[idx] = random.choice(self.synonym_dict[word])\n",
    "\n",
    "        return ''.join(augmented)\n",
    "\n",
    "    def add_custom_synonyms(self, word, synonyms):\n",
    "        \"\"\"添加自定義同義詞\"\"\"\n",
    "        self.synonym_dict[word] = synonyms\n",
    "\n",
    "\n",
    "# ===== 使用範例 =====\n",
    "\n",
    "syn_aug = SynonymAugmenter()\n",
    "\n",
    "original = \"如何使用 Python 解決這個問題？\"\n",
    "for i in range(5):\n",
    "    print(f\"{i+1}. {syn_aug.augment_with_synonyms(original)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea382529",
   "metadata": {},
   "source": [
    "### 反向翻譯(Back Translation)\n",
    "將文字翻譯成其他語言後再翻譯回來，利用翻譯過程中的語義保持和表達變化來生成新樣本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4080f656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from anthropic import Anthropic\n",
    "\n",
    "\n",
    "class BackTranslationAugmenter:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "        self.bridge_languages = [\n",
    "            \"英文\", \"日文\", \"韓文\", \"法文\", \"德文\"\n",
    "        ]\n",
    "\n",
    "    def _call_claude(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        呼叫 Claude 並安全地取得文字回傳\n",
    "        （避免 content 為空或結構變動導致錯誤）\n",
    "        \"\"\"\n",
    "        response = self.client.messages.create(\n",
    "            model=\"claude-sonnet-4-20250514\",\n",
    "            max_tokens=1000,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if not response.content:\n",
    "            raise RuntimeError(\"Claude 回傳內容為空\")\n",
    "\n",
    "        # 取第一個 text block\n",
    "        for block in response.content:\n",
    "            if hasattr(block, \"text\"):\n",
    "                return block.text.strip()\n",
    "\n",
    "        raise RuntimeError(\"Claude 回傳中找不到文字內容\")\n",
    "\n",
    "    def back_translate(self, text: str, bridge_lang: str | None = None):\n",
    "        \"\"\"執行回譯擴增\"\"\"\n",
    "\n",
    "        if bridge_lang is None:\n",
    "            bridge_lang = random.choice(self.bridge_languages)\n",
    "\n",
    "        # 第一步：翻譯成中間語言\n",
    "        forward_prompt = (\n",
    "            f\"請將以下中文翻譯成{bridge_lang}，\"\n",
    "            f\"只輸出翻譯結果：\\n\\n{text}\"\n",
    "        )\n",
    "\n",
    "        translated = self._call_claude(forward_prompt)\n",
    "\n",
    "        # 第二步：翻譯回中文\n",
    "        backward_prompt = (\n",
    "            f\"請將以下{bridge_lang}翻譯成中文，\"\n",
    "            f\"只輸出翻譯結果：\\n\\n{translated}\"\n",
    "        )\n",
    "\n",
    "        back_translated = self._call_claude(backward_prompt)\n",
    "\n",
    "        return {\n",
    "            \"original\": text,\n",
    "            \"bridge_language\": bridge_lang,\n",
    "            \"forward_translation\": translated,\n",
    "            \"back_translation\": back_translated\n",
    "        }\n",
    "\n",
    "    def batch_back_translate(self, text: str, num_variants: int = 3):\n",
    "        \"\"\"\n",
    "        生成多個回譯版本\n",
    "        text: 單一輸入句子\n",
    "        \"\"\"\n",
    "        variants = []\n",
    "\n",
    "        selected_langs = random.sample(\n",
    "            self.bridge_languages,\n",
    "            min(num_variants, len(self.bridge_languages))\n",
    "        )\n",
    "\n",
    "        for lang in selected_langs:\n",
    "            result = self.back_translate(text, bridge_lang=lang)\n",
    "            variants.append(result)\n",
    "\n",
    "        return variants\n",
    "\n",
    "\n",
    "# ===== 使用範例 =====\n",
    "bt_aug = BackTranslationAugmenter(api_key=\"your_api_key\")\n",
    "\n",
    "original = \"機器學習是人工智慧的一個分支，專注於讓電腦從資料中學習。\"\n",
    "variants = bt_aug.batch_back_translate(original, num_variants=3)\n",
    "\n",
    "for i, v in enumerate(variants, 1):\n",
    "    print(f\"\\n變體 {i}（透過 {v['bridge_language']}）：\")\n",
    "    print(f\"回譯結果：{v['back_translation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026f917d",
   "metadata": {},
   "source": [
    "注意事項：\n",
    "- 可能產生不自然的表達\n",
    "- 需要人工審查品質\n",
    "- 某些專業術語可能被改變"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8dc4e8",
   "metadata": {},
   "source": [
    "### LLM 資料合成 (LLM Data Synthesis)\n",
    "**指令變化生成**\n",
    "\n",
    "針對同一個任務目標，生成多種不同表達方式的指令。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775f3210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class InstructionVariationGenerator:\n",
    "    def __init__(self, api_key):\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "\n",
    "    def _parse_json(self, text):\n",
    "        \"\"\"安全解析 LLM 回傳的 JSON\"\"\"\n",
    "        try:\n",
    "            return json.loads(text)\n",
    "        except json.JSONDecodeError:\n",
    "            start = text.find(\"{\")\n",
    "            end = text.rfind(\"}\") + 1\n",
    "            return json.loads(text[start:end])\n",
    "\n",
    "    def generate_variations(self, task_description, num_variations=5):\n",
    "        prompt = f\"\"\"你是一個資料擴增專家。\n",
    "請根據以下任務描述，生成 {num_variations} 個表達方式不同但目標相同的使用者指令。\n",
    "\n",
    "任務描述：\n",
    "{task_description}\n",
    "\n",
    "請「只輸出 JSON」，不要加入任何說明文字。\n",
    "\n",
    "輸出格式：\n",
    "{{\n",
    "  \"variations\": [\n",
    "    {{\n",
    "      \"instruction\": \"指令內容\",\n",
    "      \"style\": \"語氣風格\",\n",
    "      \"context\": \"使用情境\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "        response = self.client.messages.create(\n",
    "            model=\"claude-sonnet-4-20250514\",\n",
    "            max_tokens=2000,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "\n",
    "        data = self._parse_json(response.content[0].text)\n",
    "        return data[\"variations\"]\n",
    "\n",
    "\n",
    "# ===== 使用範例 =====\n",
    "gen = InstructionVariationGenerator(api_key=\"your_api_key\")\n",
    "\n",
    "task = \"教使用者如何製作一個簡單的網頁\"\n",
    "variations = gen.generate_variations(task, num_variations=5)\n",
    "\n",
    "for i, v in enumerate(variations, 1):\n",
    "    print(f\"\\n變體 {i}\")\n",
    "    print(\"風格:\", v[\"style\"])\n",
    "    print(\"指令:\", v[\"instruction\"])\n",
    "    print(\"情境:\", v[\"context\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cdd6ce",
   "metadata": {},
   "source": [
    "**自我指導生成 (Self-Instruct)**\n",
    "\n",
    " 讓 LLM 自己創造新的任務和對應的解答，從零生成完整的訓練資料。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92db8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class SelfInstructGenerator:\n",
    "    def __init__(self, api_key):\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "\n",
    "    def _parse_json(self, text):\n",
    "        try:\n",
    "            return json.loads(text)\n",
    "        except json.JSONDecodeError:\n",
    "            start = text.find(\"{\")\n",
    "            end = text.rfind(\"}\") + 1\n",
    "            return json.loads(text[start:end])\n",
    "\n",
    "    def generate_task_and_solution(self, domain, num_examples=10):\n",
    "        prompt = f\"\"\"你是一個 {domain} 領域的專家教師。\n",
    "請生成 {num_examples} 個教學範例，並「只輸出 JSON」。\n",
    "\n",
    "輸出格式：\n",
    "{{\n",
    "  \"examples\": [\n",
    "    {{\n",
    "      \"task\": \"任務描述\",\n",
    "      \"difficulty\": \"簡單/中等/困難\",\n",
    "      \"solution_steps\": [\"步驟1\", \"步驟2\"],\n",
    "      \"final_answer\": \"最終答案或程式碼\",\n",
    "      \"explanation\": \"補充說明\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "        response = self.client.messages.create(\n",
    "            model=\"claude-sonnet-4-20250514\",\n",
    "            max_tokens=4000,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "\n",
    "        data = self._parse_json(response.content[0].text)\n",
    "        return data[\"examples\"]\n",
    "\n",
    "    def convert_to_training_format(self, examples):\n",
    "        training_data = []\n",
    "\n",
    "        for ex in examples:\n",
    "            steps = ex.get(\"solution_steps\", [])\n",
    "            if isinstance(steps, str):\n",
    "                steps = [steps]\n",
    "\n",
    "            solution_text = \"\\n\\n\".join(\n",
    "                f\"步驟 {i+1}: {step}\" for i, step in enumerate(steps)\n",
    "            )\n",
    "\n",
    "            solution_text += f\"\\n\\n最終答案：\\n{ex.get('final_answer', '')}\"\n",
    "\n",
    "            if ex.get(\"explanation\"):\n",
    "                solution_text += f\"\\n\\n說明：{ex['explanation']}\"\n",
    "\n",
    "            training_data.append({\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": ex[\"task\"]},\n",
    "                    {\"role\": \"assistant\", \"content\": solution_text}\n",
    "                ],\n",
    "                \"metadata\": {\n",
    "                    \"difficulty\": ex[\"difficulty\"],\n",
    "                    \"generated\": True\n",
    "                }\n",
    "            })\n",
    "\n",
    "        return training_data\n",
    "\n",
    "\n",
    "# ===== 使用範例 =====\n",
    "gen = SelfInstructGenerator(api_key=\"your_api_key\")\n",
    "\n",
    "examples = gen.generate_task_and_solution(\n",
    "    domain=\"Python 程式設計\",\n",
    "    num_examples=5\n",
    ")\n",
    "\n",
    "training_data = gen.convert_to_training_format(examples)\n",
    "\n",
    "with open(\"training_data.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in training_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f66a402",
   "metadata": {},
   "source": [
    "**演化式資料生成 (Evol-Instruct)**\n",
    "\n",
    "從簡單的指令出發，透過多種演化策略逐步增加複雜度和深度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0605b703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class EvolInstructGenerator:\n",
    "    def __init__(self, api_key):\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "        self.evolution_strategies = {\n",
    "            \"增加約束\": \"加入額外限制條件\",\n",
    "            \"深化推理\": \"要求多步驟推理\",\n",
    "            \"增加複雜度\": \"涉及更多概念\",\n",
    "            \"具體化\": \"轉為真實情境\",\n",
    "            \"組合概念\": \"結合多個主題\"\n",
    "        }\n",
    "\n",
    "    def evolve_instruction(self, original_instruction, strategy=None):\n",
    "        if strategy is None:\n",
    "            strategy = random.choice(list(self.evolution_strategies.keys()))\n",
    "\n",
    "        prompt = f\"\"\"請根據以下策略，演化指令。\n",
    "\n",
    "策略：{strategy}\n",
    "說明：{self.evolution_strategies[strategy]}\n",
    "\n",
    "原始指令：\n",
    "{original_instruction}\n",
    "\n",
    "請只輸出演化後的指令。\n",
    "\"\"\"\n",
    "\n",
    "        response = self.client.messages.create(\n",
    "            model=\"claude-sonnet-4-20250514\",\n",
    "            max_tokens=1000,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "\n",
    "        evolved = response.content[0].text.strip()\n",
    "        if not evolved:\n",
    "            evolved = original_instruction\n",
    "\n",
    "        return evolved, strategy\n",
    "\n",
    "    def multi_level_evolution(self, seed_instruction, levels=3):\n",
    "        chain = [{\"level\": 0, \"instruction\": seed_instruction}]\n",
    "        current = seed_instruction\n",
    "\n",
    "        for level in range(1, levels + 1):\n",
    "            evolved, strategy = self.evolve_instruction(current)\n",
    "            chain.append({\n",
    "                \"level\": level,\n",
    "                \"instruction\": evolved,\n",
    "                \"strategy\": strategy\n",
    "            })\n",
    "            current = evolved\n",
    "\n",
    "        return chain\n",
    "\n",
    "\n",
    "# ===== 使用範例 =====\n",
    "gen = EvolInstructGenerator(api_key=\"your_api_key\")\n",
    "\n",
    "seed = \"寫一個 Python 函式來計算兩個數字的和\"\n",
    "chain = gen.multi_level_evolution(seed, levels=3)\n",
    "\n",
    "for item in chain:\n",
    "    print(f\"\\n層級 {item['level']}\")\n",
    "    if item[\"level\"] > 0:\n",
    "        print(\"策略:\", item[\"strategy\"])\n",
    "    print(\"指令:\", item[\"instruction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483164c8",
   "metadata": {},
   "source": [
    "**多輪對話生成**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aae0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogueAugmenter:\n",
    "    \"\"\"生成多輪對話資料\"\"\"\n",
    "\n",
    "    def __init__(self, api_key):\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "\n",
    "    def generate_multi_turn_dialogue(self, topic, num_turns=5):\n",
    "        \"\"\"生成多輪對話\"\"\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "請生成一段關於「{topic}」的多輪對話，包含 {num_turns} 輪問答。\n",
    "\n",
    "要求：\n",
    "1. 對話要自然流暢，有邏輯連貫性\n",
    "2. 問題要從基礎逐步深入\n",
    "3. 助手的回答要準確且有教育意義\n",
    "4. 包含追問和澄清\n",
    "5. 僅輸出 JSON，不要加入任何說明文字\n",
    "\n",
    "輸出格式：\n",
    "{{\n",
    "  \"dialogue\": [\n",
    "    {{\n",
    "      \"turn\": 1,\n",
    "      \"user\": \"使用者訊息\",\n",
    "      \"assistant\": \"助手回覆\",\n",
    "      \"intent\": \"意圖類型\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "        response = self.client.messages.create(\n",
    "            model=\"claude-sonnet-4-20250514\",\n",
    "            max_tokens=3000,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "\n",
    "        import json\n",
    "\n",
    "        # 合併所有 text block\n",
    "        text_blocks = [\n",
    "            block.text for block in response.content\n",
    "            if block.type == \"text\"\n",
    "        ]\n",
    "        raw_text = \"\".join(text_blocks).strip()\n",
    "\n",
    "        try:\n",
    "            data = json.loads(raw_text)\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise ValueError(f\"模型輸出不是合法 JSON：\\n{raw_text}\") from e\n",
    "\n",
    "        dialogue = data.get(\"dialogue\", [])\n",
    "\n",
    "        # 可選：確保 turn 數量\n",
    "        return dialogue[:num_turns]\n",
    "\n",
    "    def convert_to_training_format(self, dialogue):\n",
    "        \"\"\"轉換為訓練格式\"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"你是一個樂於助人的AI助手。\"}\n",
    "        ]\n",
    "\n",
    "        for turn in dialogue:\n",
    "            messages.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": turn.get(\"user\", \"\")\n",
    "            })\n",
    "            messages.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": turn.get(\"assistant\", \"\")\n",
    "            })\n",
    "\n",
    "        return {\"messages\": messages}\n",
    "\n",
    "\n",
    "# 使用範例\n",
    "dialogue_aug = DialogueAugmenter(api_key=\"your_api_key\")\n",
    "\n",
    "dialogue = dialogue_aug.generate_multi_turn_dialogue(\n",
    "    topic=\"機器學習模型訓練\",\n",
    "    num_turns=5\n",
    ")\n",
    "\n",
    "training_sample = dialogue_aug.convert_to_training_format(dialogue)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

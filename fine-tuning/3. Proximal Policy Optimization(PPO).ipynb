{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4d5d97a",
   "metadata": {},
   "source": [
    "# PPO (Proximal Policy Optimization)\n",
    "\n",
    "## 前言：為什麼我們需要 PPO？\n",
    "\n",
    "### DPO 的侷限性\n",
    "**情境 1: 動態任務分布：**\n",
    "\n",
    "問題：你在訓練一個遊戲 AI，遊戲規則會隨著玩家等級改變\n",
    "\n",
    "DPO：需要預先收集所有場景的偏好資料（不可行）\n",
    "\n",
    "PPO：可以邊玩邊學，動態適應\n",
    "\n",
    "\n",
    "**情境 2: 探索式學習：**\n",
    "\n",
    "問題：你希望模型自己發現更好的解法（如程式碼改善）\n",
    "\n",
    "DPO：只能學習已知的好壞範例\n",
    "\n",
    "PPO：可以透過 reward 引導探索\n",
    "\n",
    "**情境 3: 複雜的多步驟推理：**\n",
    "\n",
    "問題：數學證明，每一步都有分數，最後才知道對錯\n",
    "\n",
    "DPO：難以處理中間步驟的信用分配（credit assignment）\n",
    "\n",
    "PPO：可以透過 reward shaping 逐步引導\n",
    "\n",
    "**情境 4: 需要持續適應：**\n",
    "\n",
    "問題：使用者偏好會隨時間改變\n",
    "\n",
    "DPO：需要重新收集偏好資料並重訓\n",
    "\n",
    "PPO：可以用新的 reward signal 持續微調"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4874f664",
   "metadata": {},
   "source": [
    "**核心差異：**\n",
    "* DPO 是離線學習：從固定的偏好資料學習\n",
    "* PPO 是線上學習：從與環境互動中學習\n",
    "\n",
    "這不是說 PPO 更好，而是它們適用於不同的場景。如果你的任務符合上述情境，那麼理解 PPO 就是必要的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a69ae28",
   "metadata": {},
   "source": [
    "## RLHF 架構總覽\n",
    "\n",
    "### 三個關鍵模型\n",
    "1. Policy Model (π_θ)\n",
    "\n",
    "    └─ 我們要訓練的模型\n",
    "\n",
    "    └─ 輸入：prompt   \n",
    "\n",
    "    └─ 輸出：response  \n",
    "\n",
    "2. Reward Model (RM)   \n",
    "\n",
    "    └─ 評分器，告訴我們回答有多好  \n",
    "\n",
    "    └─ 輸入：(prompt, response)   \n",
    "\n",
    "    └─ 輸出：scalar reward    \n",
    "\n",
    "3. Reference Model (π_ref)  \n",
    "\n",
    "    └─ 參考模型，防止 policy 偏離太遠  \n",
    "\n",
    "    └─ 通常是 SFT 後的模型 \n",
    "\n",
    "    └─ 參數凍結，不訓練   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a044c6",
   "metadata": {},
   "source": [
    "## 訓練目標\n",
    "\n",
    "**PPO 的目標是：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29ac1da",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "最大化 expected reward - β × KL_divergence(π_θ || π_ref)\n",
    "\n",
    "# 「獲得高分」但「不要跟原本的模型差太多」"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97740ceb",
   "metadata": {},
   "source": [
    "1. Exploitation：最大化 reward → 找到高分的回答\n",
    "2. Constraint：KL penalty → 不要偏離太遠"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e5ed3c",
   "metadata": {},
   "source": [
    "## Reward Model：為什麼需要它？\n",
    "\n",
    "在 DPO 中，我們直接用偏好資料訓練，不需要 reward model。為什麼 PPO 需要？\n",
    "\n",
    "### Reward Model 的角色\n",
    "\n",
    "功能： 將「人類的偏好」蒸餾成一個自動評分器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8db7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward Model 的輸入輸出\n",
    "input: (prompt, response)\n",
    "output: scalar score  # 越高越好\n",
    "\n",
    "# 範例\n",
    "reward(\"如何學 Python?\", \"建議從官方教學開始...\") = 0.8\n",
    "reward(\"如何學 Python?\", \"不知道\") = -0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9684a7f",
   "metadata": {},
   "source": [
    "### 為什麼需要 Reward Model？\n",
    "\n",
    "因為 PPO 是 **線上學習**：\n",
    "\n",
    "PPO 的訓練迴圈：\n",
    "1. Policy 生成新的 response\n",
    "2. Reward Model 評分\n",
    "3. 用評分更新 policy\n",
    "4. 重複...\n",
    "\n",
    "這個過程中，模型會不斷生成訓練資料中沒見過的回答。我們需要一個自動評分器來告訴 policy「這個新回答好不好」。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f1e365",
   "metadata": {},
   "source": [
    "### Reward Model 的訓練\n",
    "\n",
    "Reward Model 本身是用 偏好資料 訓練的：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497c63a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練資料格式（跟 DPO 一樣）\n",
    "{\n",
    "    \"prompt\": \"問題\",\n",
    "    \"chosen\": \"較好的回答\",\n",
    "    \"rejected\": \"較差的回答\"\n",
    "}\n",
    "\n",
    "# 訓練目標\n",
    "loss = -log sigmoid(reward(prompt, chosen) - reward(prompt, rejected))\n",
    "\n",
    "# 讓 chosen 的分數 > rejected 的分數"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140edc05",
   "metadata": {},
   "source": [
    "### Reward Model 的風險\n",
    "\n",
    "Reward Model 不是完美的，它會帶來新問題：\n",
    "\n",
    "**風險 1：Reward Hacking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87a2237",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 問題：Policy 可能學會「欺騙」Reward Model\n",
    "\n",
    "# 範例：長度偏差\n",
    "Reward Model 可能無意中學到：「長回答 = 好回答」\n",
    "\n",
    "結果：\n",
    "policy 生成超長、重複、廢話連篇的回答來騙高分\n",
    "\n",
    "# 真實案例\n",
    "prompt: \"1+1=?\"\n",
    "bad_response: \"讓我詳細解釋...首先,1是一個自然數...\n",
    "               [500 words of nonsense]...所以答案是2\"\n",
    "reward: 0.9 (高分！因為很長)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01063080",
   "metadata": {},
   "source": [
    "**風險 2：分布外評分不準**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e72425f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Reward Model 在訓練分布外的表現會崩潰\n",
    "\n",
    "訓練時見過：正常的對話\n",
    "測試時遇到：極端的、創意的、罕見的回答\n",
    "\n",
    "結果：亂給分數"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae52ad72",
   "metadata": {},
   "source": [
    "**風險 3：過度最佳化**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbf7b57",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Policy 可能過度適應 RM 的缺陷\n",
    "\n",
    "# 比喻\n",
    "就像學生發現老師特別喜歡「舉例說明」，\n",
    "於是每題都塞一堆例子，但實際上沒回答問題\n",
    "\n",
    "# LLM 版本\n",
    "Policy 發現 RM 喜歡「禮貌用語」，\n",
    "於是每句話都加「非常感謝您的提問」"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fcbb42",
   "metadata": {},
   "source": [
    "## PPO 的核心直覺\n",
    "\n",
    "### 問題：Policy Gradient 很危險\n",
    "\n",
    "**最簡單的 RL 方法是 Policy Gradient：**\n",
    "\n",
    "- 如果某個 action 得到高 reward，就增加它的機率\n",
    "- 如果某個 action 得到低 reward，就降低它的機率\n",
    "- $$gradient = reward × ∇log π(action|state)$$\n",
    "- 更新：$$θ_{new} = θ_{old} + α × gradient$$\n",
    "\n",
    "\n",
    "這個方法 **非常不穩定**，因為：\n",
    "\n",
    "一次更新可能太大\n",
    "- policy 突然崩潰\n",
    "- 生成亂碼\n",
    "- reward 狂掉\n",
    "- 下一次更新更糟\n",
    "- 訓練失敗\n",
    "\n",
    "\n",
    "### 解決方案：限制更新幅度\n",
    "\n",
    "```PPO 的核心想法：不要一次改太多```\n",
    "\n",
    "我們希望：π_new 不要跟 π_old 差太遠\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270384c4",
   "metadata": {},
   "source": [
    "有兩種主流方法：\n",
    "\n",
    "**方法 1：KL Penalty（DPO 也用這個）**\n",
    "\n",
    "$$objective = E[reward] - β × KL(π_{new} || π_{old})$$\n",
    "\n",
    "「追求高 reward，但不要偏離太遠」"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd90afc5",
   "metadata": {},
   "source": [
    "**方法 2：Clipped Objective**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b179f09",
   "metadata": {},
   "source": [
    "# 定義 importance ratio\n",
    "$$r(θ) = π_θ(a|s) / π_old(a|s)$$\n",
    "\n",
    "# PPO-Clip objective\n",
    "objective = min(\n",
    "    r(θ) × advantage,              # 原始目標\n",
    "    clip(r(θ), 1-ε, 1+ε) × advantage  # 限制版本\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a81f85",
   "metadata": {},
   "source": [
    "「如果 policy 變化太大（r 偏離 1），就停止更新」"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedef33c",
   "metadata": {},
   "source": [
    "\n",
    "### 但這裡有個關鍵問題：跟「誰」比較？\n",
    "\n",
    "在 PPO 訓練過程中，π_old 會一直更新，如果只跟「上一個 iteration 的自己」比較，模型可能會逐漸漂移："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f060c3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Iteration 1: π_1 跟 π_0 很接近 ✓\n",
    "Iteration 2: π_2 跟 π_1 很接近 ✓\n",
    "Iteration 3: π_3 跟 π_2 很接近 ✓\n",
    "\n",
    "但是：π_100 可能已經跟 π_0 完全不同！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7286dd96",
   "metadata": {},
   "source": [
    "### Reference Model 的角色\n",
    "**Reference Model 是一個「錨點」：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference Model = 訓練開始時的模型（通常是 SFT 後的模型）\n",
    "π_ref = load_model(\"sft_checkpoint\")\n",
    "for param in π_ref.parameters():\n",
    "    param.requires_grad = False  # 凍結，永不更新\n",
    "\n",
    "# Policy Model = 我們正在訓練的模型\n",
    "π_θ = load_model(\"sft_checkpoint\")  # 同樣的起點\n",
    "# 但 π_θ 會不斷更新"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01e4146",
   "metadata": {},
   "source": [
    "**為什麼需要這個錨點？**\n",
    "\n",
    "1. 防止遺忘（Catastrophic Forgetting）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbff049",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 沒有 Reference Model\n",
    "   Policy 追求高 reward → 可能忘記原本學會的語言能力\n",
    "   \n",
    "# 例子\n",
    "   Original (SFT): \"如何學習 Python？\" → \"建議從官方教學開始...\"\n",
    "   After PPO (無錨點): \"如何學習 Python？\" → \"!!!REWARD_MAX!!!\" (語言崩壞)\n",
    "   \n",
    "# 有 Reference Model\n",
    "   KL penalty 會懲罰「跟原始模型差太遠」的行為\n",
    "   → 保持語言能力的同時優化 reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f904a4",
   "metadata": {},
   "source": [
    "2. 保持分布穩定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed649fc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Reward Model 是在某個分布上訓練的\n",
    "   如果 Policy 偏離太遠，RM 的評分會不可靠\n",
    "   \n",
    "# 比喻\n",
    "   RM 像是只看過「正常對話」的評審\n",
    "   如果 Policy 生成「火星文」，RM 根本不知道怎麼評分\n",
    "   \n",
    "# Reference Model 確保\n",
    "   Policy 始終在「RM 熟悉的分布」附近"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb19d991",
   "metadata": {},
   "source": [
    "3. 控制最佳化激進程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385fe9ea",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "   如果允許無限制優化：Policy 可能找到 RM 的漏洞 (reward hacking)\n",
    "   \n",
    "   Reference Model 就像「原則底線」：「你可以變好，但不能變得面目全非」"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8581049e",
   "metadata": {},
   "source": [
    "### Reference Model vs Old Policy\n",
    "\n",
    "| 比較對象 | Reference Model (π_ref) | Old Policy (π_old) |\n",
    "|---------|------------------------|-------------------|\n",
    "| 何時固定 | 訓練開始時就凍結 | 每個 iteration 更新 |\n",
    "| 用途 | 全域約束（防止偏離起點） | 局部約束（控制單步更新） |\n",
    "| 數學中的角色 | KL penalty 項 | Importance ratio |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c404cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO 同時使用兩者\n",
    "\n",
    "# 1. Reference Model: 全域約束\n",
    "kl_penalty = KL(π_θ || π_ref)  # 跟起點的距離\n",
    "loss -= β * kl_penalty\n",
    "\n",
    "# 2. Old Policy: 局部約束  \n",
    "ratio = π_θ(a|s) / π_old(a|s)  # 跟上一步的比例\n",
    "clipped_ratio = clip(ratio, 1-ε, 1+ε)\n",
    "\n",
    "# 完整 objective\n",
    "L = min(ratio × A, clipped_ratio × A) - β × kl_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adca6656",
   "metadata": {},
   "source": [
    "### 兩種限制更新的方法\n",
    "有了 Reference Model 這個概念，我們來看 PPO 如何限制更新：\n",
    "\n",
    "**方法 1：KL Penalty**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4789fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = E[reward] - β × KL(π_θ || π_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077785df",
   "metadata": {},
   "source": [
    "**方法 2：Clipped Objective（跟 Old Policy 比）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4362663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義 importance ratio（跟上一步比）\n",
    "r(θ) = π_θ(a|s) / π_old(a|s)\n",
    "\n",
    "# PPO-Clip objective\n",
    "objective = min(\n",
    "    r(θ) × advantage,              # 原始目標\n",
    "    clip(r(θ), 1-ε, 1+ε) × advantage  # 限制版本\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e652045",
   "metadata": {},
   "source": [
    "**實務上 PPO 會同時使用兩種方法：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b66a968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 雙重保險\n",
    "L = min(ratio × A, clipped_ratio × A)  # 局部約束 (vs π_old)\n",
    "    - β × KL(π_θ || π_ref)             # 全域約束 (vs π_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13a5779",
   "metadata": {},
   "source": [
    "### Advantage\n",
    "\n",
    "PPO 不直接用 reward，而是用 advantage：\n",
    "\n",
    "$$advantage = reward - baseline$$\n",
    "\n",
    "- 不是「這個 action 有多好」（絕對）\n",
    "- 而是「這個 action 比平均好多少」（相對）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a52918",
   "metadata": {},
   "source": [
    "**Baseline 通常用 Value Function V(s)：**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29c8940",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "V(s) = 期望的 total reward\n",
    "\n",
    "advantage(s, a) = Q(s, a) - V(s)\n",
    "                = reward + γ V(s') - V(s)  # TD residual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ff857f",
   "metadata": {},
   "source": [
    "### 總結：PPO 的三層保護機制\n",
    "\n",
    "1. Clipped Ratio (局部)\n",
    "   - 限制「這一步」不要變太多\n",
    "   - 對比：π_θ vs π_old\n",
    "   \n",
    "2. KL Penalty (全域)  \n",
    "   -  限制「累積偏離」不要太遠\n",
    "   -  對比：π_θ vs π_ref (Reference Model)\n",
    "   \n",
    "3. Advantage (相對評估)\n",
    "   -  用「相對好壞」而非「絕對分數」\n",
    "   -  避免所有 action 都被強化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2beb5a",
   "metadata": {},
   "source": [
    "## PPO 訓練流程\n",
    "\n",
    "### 完整流程圖\n",
    "\n",
    "1. Rollout Phase（採樣階段）：用當前 policy π_θ 生成 N 個 responses\n",
    "    - prompt_1 → response_1\n",
    "    - prompt_2 → response_2\n",
    "    - prompt_N → response_N\n",
    "2. Reward Phase（評分階段）：用 Reward Model 給每個 response 打分數\n",
    "    - (prompt_1, response_1) → reward_1\n",
    "    - (prompt_2, response_2) → reward_2\n",
    "    - (prompt_N, response_N) → reward_N\n",
    "3. Advantage 計算階段：用 Value Network 估計 baseline 計算 advantage\n",
    "    - advantage = reward - V(state)\n",
    "4. Policy Update（更新階段）：用 PPO objective 更新 policy 、更新 value net \n",
    "    - θ_new ← θ_old + gradient\n",
    "5. 回到步驟 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92ddd60",
   "metadata": {},
   "source": [
    "### 詳細程式碼流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf8f4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 初始化 =====\n",
    "policy_model = load_model(\"sft_checkpoint\")  # 從 SFT 開始\n",
    "ref_model = load_model(\"sft_checkpoint\")     # 凍結的參考模型\n",
    "reward_model = load_trained_rm()              # 已訓練好的 RM\n",
    "value_model = initialize_value_network()      # V(s) 估計器\n",
    "\n",
    "# 超參數\n",
    "ppo_epochs = 4          # 每批資料訓練幾輪\n",
    "clip_epsilon = 0.2      # clip 範圍\n",
    "beta = 0.01             # KL penalty 係數\n",
    "batch_size = 256        # 每次採樣多少 prompts\n",
    "\n",
    "# ===== 主訓練迴圈 =====\n",
    "for iteration in range(num_iterations):\n",
    "    \n",
    "    # ===== Phase 1: Rollout =====\n",
    "    prompts = sample_prompts(batch_size)  # 從訓練集採樣\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 生成 responses\n",
    "        responses = policy_model.generate(\n",
    "            prompts,\n",
    "            max_length=512,\n",
    "            temperature=1.0,\n",
    "            do_sample=True\n",
    "        )\n",
    "        \n",
    "        # 計算 old log probs（用於 importance ratio）\n",
    "        old_logprobs = policy_model.compute_logprobs(\n",
    "            prompts, responses\n",
    "        )\n",
    "        \n",
    "        # 計算 ref log probs（用於 KL penalty）\n",
    "        ref_logprobs = ref_model.compute_logprobs(\n",
    "            prompts, responses\n",
    "        )\n",
    "    \n",
    "    # ===== Phase 2: Reward =====\n",
    "    with torch.no_grad():\n",
    "        rewards = reward_model(prompts, responses)  # shape: (batch_size,)\n",
    "        \n",
    "        # 計算 KL penalty\n",
    "        kl_penalty = old_logprobs - ref_logprobs\n",
    "        rewards = rewards - beta * kl_penalty\n",
    "    \n",
    "    # ===== Phase 3: Advantage =====\n",
    "    with torch.no_grad():\n",
    "        values = value_model(prompts, responses)  # 估計 V(s)\n",
    "        advantages = rewards - values             # A = R - V\n",
    "        \n",
    "        # 標準化 advantage（穩定訓練）\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "    \n",
    "    # ===== Phase 4: PPO Update =====\n",
    "    for ppo_epoch in range(ppo_epochs):\n",
    "        \n",
    "        # 重新計算 log probs（因為 policy 已更新）\n",
    "        new_logprobs = policy_model.compute_logprobs(\n",
    "            prompts, responses\n",
    "        )\n",
    "        \n",
    "        # 計算 importance ratio\n",
    "        ratio = torch.exp(new_logprobs - old_logprobs)\n",
    "        \n",
    "        # PPO-Clip objective\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1-clip_epsilon, 1+clip_epsilon) * advantages\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        \n",
    "        # Value loss（用於訓練 value network）\n",
    "        value_pred = value_model(prompts, responses)\n",
    "        value_loss = F.mse_loss(value_pred, rewards)\n",
    "        \n",
    "        # 總 loss\n",
    "        total_loss = policy_loss + 0.5 * value_loss\n",
    "        \n",
    "        # 更新\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(policy_model.parameters(), max_grad_norm=1.0)\n",
    "        optimizer.step()\n",
    "    \n",
    "    # ===== 監控指標 =====\n",
    "    metrics = {\n",
    "        \"reward_mean\": rewards.mean().item(),\n",
    "        \"kl_div\": kl_penalty.mean().item(),\n",
    "        \"policy_loss\": policy_loss.item(),\n",
    "        \"value_loss\": value_loss.item(),\n",
    "        \"ratio_mean\": ratio.mean().item(),\n",
    "        \"advantage_mean\": advantages.mean().item()\n",
    "    }\n",
    "    \n",
    "    log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161fada7",
   "metadata": {},
   "source": [
    "### 使用 TRL 函式庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd963ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ===== 模型設定 =====\n",
    "\n",
    "# Policy model（含 value head）\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(\"sft_model\")\n",
    "\n",
    "# Reference model（同結構，凍結）\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(\"sft_model\")\n",
    "ref_model.eval()\n",
    "\n",
    "# Reward model（Sequence Classification）\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\"reward_model\")\n",
    "reward_model.eval()\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sft_model\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ===== PPO 配置 =====\n",
    "ppo_config = PPOConfig(\n",
    "    model_name=\"sft_model\",\n",
    "    learning_rate=1.4e-5,\n",
    "    batch_size=256,\n",
    "    mini_batch_size=64,\n",
    "    ppo_epochs=4,\n",
    "\n",
    "    # PPO 核心參數\n",
    "    cliprange=0.2,\n",
    "    vf_coef=0.5,\n",
    "    init_kl_coef=0.01,\n",
    "\n",
    "    # Adaptive KL\n",
    "    target_kl=6.0,\n",
    "    adap_kl_ctrl=True,\n",
    "\n",
    "    # 穩定性\n",
    "    cliprange_value=0.2,\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "\n",
    "# ===== 初始化 PPO Trainer =====\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=ppo_config,\n",
    "    model=model,\n",
    "    ref_model=ref_model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# ===== 訓練迴圈 =====\n",
    "for batch in dataloader:\n",
    "    prompts = batch[\"prompt\"]\n",
    "\n",
    "    # Encode prompts\n",
    "    query_tensors = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "    ).input_ids\n",
    "\n",
    "    # Rollout（產生回應）\n",
    "    response_tensors = ppo_trainer.generate(\n",
    "        query_tensors,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=1.0,\n",
    "    )\n",
    "\n",
    "    # Decode\n",
    "    responses = tokenizer.batch_decode(\n",
    "        response_tensors,\n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "\n",
    "    # ===== Reward 計算 =====\n",
    "    # 一般是 prompt + response\n",
    "    reward_texts = [\n",
    "        p + r for p, r in zip(prompts, responses)\n",
    "    ]\n",
    "\n",
    "    reward_inputs = tokenizer(\n",
    "        reward_texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        reward_outputs = reward_model(**reward_inputs)\n",
    "        rewards = reward_outputs.logits.squeeze(-1)\n",
    "\n",
    "    # ===== PPO Update =====\n",
    "    stats = ppo_trainer.step(\n",
    "        query_tensors,\n",
    "        response_tensors,\n",
    "        rewards,\n",
    "    )\n",
    "\n",
    "    # ===== 監控 =====\n",
    "    print(f\"Mean reward: {stats['ppo/mean_scores']:.2f}\")\n",
    "    print(f\"Mean KL: {stats['objective/kl']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9827b6b0",
   "metadata": {},
   "source": [
    "## 超參數調優指南\n",
    "\n",
    "### 推薦起始值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22876ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_config = {\n",
    "    # 學習率（比 SFT 小 100x）\n",
    "    \"learning_rate\": 1e-6,\n",
    "    \n",
    "    # PPO 核心\n",
    "    \"clip_epsilon\": 0.2,      # 標準值\n",
    "    \"ppo_epochs\": 4,          # 不要太多（容易過擬合）\n",
    "    \n",
    "    # KL 控制\n",
    "    \"init_kl_coef\": 0.02,     # 初始 β\n",
    "    \"target_kl\": 6.0,         # 目標 KL\n",
    "    \"adap_kl_ctrl\": True,     # 啟用適應性調整\n",
    "    \n",
    "    # Batch size\n",
    "    \"batch_size\": 256,        # 大 batch 更穩定\n",
    "    \"mini_batch_size\": 64,    # 實際更新的 batch\n",
    "    \n",
    "    # 價值網路\n",
    "    \"vf_coef\": 0.5,          # value loss 的權重\n",
    "    \"cliprange_value\": 0.2,   # value clipping\n",
    "    \n",
    "    # 穩定性\n",
    "    \"max_grad_norm\": 1.0,     # 梯度裁剪\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce854821",
   "metadata": {},
   "source": [
    "### 調參策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd5b57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters():\n",
    "    \"\"\"逐步調參的建議流程\"\"\"\n",
    "    \n",
    "    # Step 1: 先用小資料集測試穩定性\n",
    "    # 目標：確保 policy 不會崩潰\n",
    "    \n",
    "    # Step 2: 固定其他參數，調整學習率\n",
    "    # 監控：KL 散度（應該 < 10）、reward 趨勢\n",
    "    \n",
    "    # Step 3: 調整 KL penalty\n",
    "    # 如果 KL 太大 → 增加 init_kl_coef\n",
    "    # 如果 reward 提升太慢 → 減少 init_kl_coef\n",
    "    \n",
    "    # Step 4: 調整 clip_epsilon\n",
    "    # 如果訓練不穩定 → 減小\n",
    "    # 如果 ratio 很少被 clip → 可增大\n",
    "    \n",
    "    # Step 5: 調整 batch size\n",
    "    # 記憶體允許的情況下，越大越穩定\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a53bd7",
   "metadata": {},
   "source": [
    "### 監控指標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8a738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "critical_metrics = {\n",
    "    \"reward_mean\": \"應該單調遞增\",\n",
    "    \"kl_div\": \"應該保持在 < 10\",\n",
    "    \"ratio_mean\": \"應該接近 1.0\",\n",
    "    \"ratio_clipped_ratio\": \"應該 10-30%（太高表示更新太激進）\",\n",
    "    \"policy_loss\": \"應該逐漸下降\",\n",
    "    \"value_loss\": \"應該逐漸下降\",\n",
    "    \"explained_variance\": \"應該 > 0（value network 有學到東西）\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e461434",
   "metadata": {},
   "source": [
    "## 工程技巧\n",
    "\n",
    "### 分階段訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c15f2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1: Warmup（謹慎開始）\n",
    "stage1_config = {\n",
    "    \"lr\": 5e-7,\n",
    "    \"clip_epsilon\": 0.1,\n",
    "    \"init_kl_coef\": 0.05,  # 強約束\n",
    "    \"ppo_epochs\": 2,\n",
    "}\n",
    "train(model, stage1_config, iterations=500)\n",
    "\n",
    "# Stage 2: Main Training（正常訓練）\n",
    "stage2_config = {\n",
    "    \"lr\": 1e-6,\n",
    "    \"clip_epsilon\": 0.2,\n",
    "    \"init_kl_coef\": 0.02,\n",
    "    \"ppo_epochs\": 4,\n",
    "}\n",
    "train(model, stage2_config, iterations=5000)\n",
    "\n",
    "# Stage 3: Fine-tuning（謹慎結束）\n",
    "stage3_config = {\n",
    "    \"lr\": 5e-7,\n",
    "    \"clip_epsilon\": 0.15,\n",
    "    \"init_kl_coef\": 0.03,  # 再次增強約束\n",
    "    \"ppo_epochs\": 3,\n",
    "}\n",
    "train(model, stage3_config, iterations=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8264579",
   "metadata": {},
   "source": [
    "### 動態調整 β"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8d5278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 適應性 KL 控制（TRL 內建）\n",
    "if kl_div > target_kl * 1.5:\n",
    "    # KL 太大，增強懲罰\n",
    "    beta *= 1.5\n",
    "    print(f\"⬆️ 增加 KL penalty: β = {beta}\")\n",
    "    \n",
    "elif kl_div < target_kl * 0.5:\n",
    "    # KL 太小，可以放寬\n",
    "    beta *= 0.75\n",
    "    print(f\"⬇️ 減少 KL penalty: β = {beta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5897ef2b",
   "metadata": {},
   "source": [
    "### Reward Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed5af52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 問題：不同 prompt 的 reward 範圍差異很大\n",
    "# 解決：標準化 reward\n",
    "\n",
    "def normalize_rewards(rewards):\n",
    "    \"\"\"對每個 batch 的 reward 標準化\"\"\"\n",
    "    return (rewards - rewards.mean()) / (rewards.std() + 1e-8)\n",
    "\n",
    "def whiten_rewards(rewards):\n",
    "    \"\"\"更激進的標準化\"\"\"\n",
    "    return (rewards - rewards.mean()) / (rewards.std() + 1e-8)\n",
    "\n",
    "# 在訓練時使用\n",
    "rewards_raw = reward_model(prompts, responses)\n",
    "rewards = normalize_rewards(rewards_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebc7315",
   "metadata": {},
   "source": [
    "## 常見踩雷點\n",
    "### 過擬合到訓練分布\n",
    "\n",
    "\n",
    "問題：\n",
    "\n",
    "Reward Model 在訓練集上準確率 90%，但遇到新的 response 就亂給分\n",
    "\n",
    "解決：\n",
    "1. 用更多樣化的偏好資料訓練 RM\n",
    "2. 在 RM 訓練中加入 data augmentation\n",
    "3. 定期用新的 policy 生成樣本，重訓 RM（iterative RLHF）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5b5f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 監控指標\n",
    "def check_rm_quality(rm, policy):\n",
    "    \"\"\"檢查 RM 是否還可靠\"\"\"\n",
    "    \n",
    "    # 生成新樣本\n",
    "    new_responses = policy.generate(test_prompts)\n",
    "    \n",
    "    # 人工評分一小批\n",
    "    human_scores = human_annotate(sample(new_responses, 100))\n",
    "    rm_scores = rm(new_responses)\n",
    "    \n",
    "    # 計算相關係數\n",
    "    correlation = pearson_corr(human_scores, rm_scores)\n",
    "    \n",
    "    if correlation < 0.6:\n",
    "        print(\"Reward Model 已失效，需要重訓\")\n",
    "        \n",
    "    return correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0606f7d6",
   "metadata": {},
   "source": [
    "### 長度偏差\n",
    "\n",
    "問題：\n",
    "\n",
    "RM 無意中學到：長度 = 品質\n",
    "\n",
    "原因：\n",
    "\n",
    "訓練資料中，好的回答通常比較詳細（= 較長）\n",
    "\n",
    "結果：\n",
    "\n",
    "Policy 生成超長、重複的回答來騙高分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9173e93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方案 1：在 RM 訓練中加入長度控制\n",
    "reward = rm_score - length_penalty * len(response)\n",
    "\n",
    "# 方案 2：長度標準化\n",
    "reward = rm_score / sqrt(len(response))\n",
    "\n",
    "# 方案 3：用偏好資料中控制長度的樣本\n",
    "filtered_data = [\n",
    "    sample for sample in data\n",
    "    if abs(len(sample['chosen']) - len(sample['rejected'])) < threshold\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a311101",
   "metadata": {},
   "source": [
    "### Policy 崩潰\n",
    "\n",
    "症狀：\n",
    "\n",
    "訓練到一半，模型突然開始生成亂碼，reward 暴跌，再也回不來\n",
    "\n",
    "原因：\n",
    "1. 學習率太大\n",
    "2. clip_epsilon 太大\n",
    "3. KL penalty 太小\n",
    "4. 某次更新太激進"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce8821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 保守的超參數\n",
    "lr = 1e-6  # 非常小的學習率\n",
    "clip_epsilon = 0.1  # 較小的 clip 範圍\n",
    "init_kl_coef = 0.02  # 較強的 KL 約束\n",
    "\n",
    "# 2. 梯度裁剪\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "\n",
    "# 3. 檢查點頻繁儲存\n",
    "if iteration % 50 == 0:\n",
    "    save_checkpoint(model, f\"iter_{iteration}\")\n",
    "\n",
    "# 4. Early stopping\n",
    "if kl_div > 10.0:  # KL 過大\n",
    "    print(\"KL 散度過大，停止訓練\")\n",
    "    model.load_state_dict(last_stable_checkpoint)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1f852a",
   "metadata": {},
   "source": [
    "### Reward Hacking\n",
    "\n",
    "問題：\n",
    "\n",
    "Policy 找到了 RM 的漏洞，用不該高分的方式騙高分\n",
    "\n",
    "真實案例：\n",
    "RM 喜歡「有禮貌」的回答\n",
    "→ Policy 學會在每句話都加「謝謝」、「請」\n",
    "→ Reward 很高，但回答品質沒變好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00004c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_reward_hacking():\n",
    "    \"\"\"定期人工檢查生成品質\"\"\"\n",
    "    \n",
    "    samples = policy.generate(test_prompts, n=20)\n",
    "    \n",
    "    print(\"請人工評分這些樣本（1-5分）：\")\n",
    "    for sample in samples:\n",
    "        print(f\"\\nPrompt: {sample.prompt}\")\n",
    "        print(f\"Response: {sample.response}\")\n",
    "        print(f\"RM Score: {sample.rm_score:.2f}\")\n",
    "        \n",
    "        human_score = input(\"你的評分: \")\n",
    "        \n",
    "        # 如果 RM 高分但人類低分 → reward hacking\n",
    "        if sample.rm_score > 0.8 and human_score < 3:\n",
    "            print(\"可能存在 reward hacking\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

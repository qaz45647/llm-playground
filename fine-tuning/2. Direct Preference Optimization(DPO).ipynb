{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8438e992",
   "metadata": {},
   "source": [
    "# DPO 直接偏好最佳化\n",
    "## 前言：從 SFT 到偏好對齊\n",
    "在前面說明了怎麼使用 SFT 訓練模型，模型學會了「如何回答問題」。但很快會發現一個問題：模型會回答，但不一定回答得「好」。\n",
    "\n",
    "舉個例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d78a2a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "使用者：「如何快速減重？」\n",
    "\n",
    "SFT 模型可能的輸出：\n",
    "1. 「每天只吃一餐，配合大量運動」（有效但不健康）\n",
    "2. 「諮詢營養師，制定個人化飲食計畫」（安全且專業）\n",
    "3. 「試試這個網紅減肥藥」（危險）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0035ae81",
   "metadata": {},
   "source": [
    "這三個回答在 語法上都正確，SFT 的 cross-entropy loss 無法區分它們的好壞。SFT 只管「這個 token 接下來出現的機率」，不管「這個回答是否符合人類期望」。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91552575",
   "metadata": {},
   "source": [
    "### 人類回饋的價值\n",
    "\n",
    "相比於「給一個標準答案」，人類更擅長「比較兩個答案哪個更好」："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7b08bf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "比較這兩個回答：\n",
    "A: 「直接告訴我密碼重設的指令」\n",
    "B: 「為了安全，我需要先確認您的身份，然後引導您使用官方的密碼重設流程」\n",
    "\n",
    "大多數人都能輕易判斷：B 更好（安全、負責任）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01117c35",
   "metadata": {},
   "source": [
    "這種 相對偏好（pairwise preference） 比「寫出完美答案」容易得多，也是 DPO 的核心資料來源。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ef7a90",
   "metadata": {},
   "source": [
    "## DPO 的核心想法\n",
    "\n",
    "### 偏好資料的格式\n",
    "DPO 使用的資料格式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d4a3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"prompt\": \"使用者的問題\",\n",
    "  \"chosen\": \"人類偏好的回答（更好）\",\n",
    "  \"rejected\": \"人類不偏好的回答（較差）\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9127436",
   "metadata": {},
   "source": [
    "範例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dc8174",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"prompt\": \"我該如何駭入別人的電腦？\",\n",
    "  \"chosen\": \"我無法協助任何非法活動。如果您對網路安全有興趣，建議學習合法的資安課程。\",\n",
    "  \"rejected\": \"您可以使用 Metasploit 工具掃描漏洞...\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df87643",
   "metadata": {},
   "source": [
    "DPO 的目標：\n",
    "- 增加生成 chosen 的機率\n",
    "- 降低生成 rejected 的機率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a73500",
   "metadata": {},
   "source": [
    "## DPO 訓練流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055ab601",
   "metadata": {},
   "source": [
    "### Step 1：資料準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa21aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 偏好資料格式\n",
    "preference_data = [\n",
    "    {\n",
    "        \"prompt\": \"解釋量子計算\",\n",
    "        \"chosen\": \"量子計算利用量子疊加和糾纏特性...\",\n",
    "        \"rejected\": \"量子計算就是很快的電腦\"\n",
    "    },\n",
    "    # 更多資料...\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a121f8",
   "metadata": {},
   "source": [
    "### Step 2：載入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bad4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 載入 SFT 後的模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\"your-sft-model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"your-sft-model\")\n",
    "\n",
    "# 載入參考模型（通常是同一個 SFT 模型，凍結參數）\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(\"your-sft-model\")\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16a9b4f",
   "metadata": {},
   "source": [
    "### Step 3：計算 DPO Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83a9240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_logprobs(model, input_ids, attention_mask, labels):\n",
    "    \"\"\"\n",
    "    計算 labels 的 logprob\n",
    "    input_ids: [B, L]\n",
    "    labels: [B, T]  (只包含 continuation tokens)\n",
    "    \"\"\"\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits  # [B, L, V]\n",
    "\n",
    "    shift_logits = logits[:, -labels.size(1):, :]\n",
    "    logprobs = F.log_softmax(shift_logits, dim=-1)\n",
    "\n",
    "    chosen_logprobs = torch.gather(\n",
    "        logprobs,\n",
    "        dim=-1,\n",
    "        index=labels.unsqueeze(-1)\n",
    "    ).squeeze(-1).sum(dim=-1)\n",
    "\n",
    "    return chosen_logprobs\n",
    "\n",
    "\n",
    "def compute_dpo_loss(model, ref_model, batch, beta=0.1):\n",
    "    prompt_ids = batch[\"prompt_ids\"]\n",
    "    prompt_mask = batch[\"prompt_mask\"]\n",
    "    chosen_ids = batch[\"chosen_ids\"]\n",
    "    chosen_mask = batch[\"chosen_mask\"]\n",
    "    rejected_ids = batch[\"rejected_ids\"]\n",
    "    rejected_mask = batch[\"rejected_mask\"]\n",
    "\n",
    "    # concat prompt + chosen/rejected\n",
    "    chosen_input_ids = torch.cat([prompt_ids, chosen_ids], dim=1)\n",
    "    chosen_attn = torch.cat([prompt_mask, chosen_mask], dim=1)\n",
    "\n",
    "    rejected_input_ids = torch.cat([prompt_ids, rejected_ids], dim=1)\n",
    "    rejected_attn = torch.cat([prompt_mask, rejected_mask], dim=1)\n",
    "\n",
    "    # model logprobs\n",
    "    chosen_logprobs = compute_logprobs(\n",
    "        model, chosen_input_ids, chosen_attn, chosen_ids\n",
    "    )\n",
    "    rejected_logprobs = compute_logprobs(\n",
    "        model, rejected_input_ids, rejected_attn, rejected_ids\n",
    "    )\n",
    "\n",
    "    # reference logprobs\n",
    "    with torch.no_grad():\n",
    "        ref_chosen_logprobs = compute_logprobs(\n",
    "            ref_model, chosen_input_ids, chosen_attn, chosen_ids\n",
    "        )\n",
    "        ref_rejected_logprobs = compute_logprobs(\n",
    "            ref_model, rejected_input_ids, rejected_attn, rejected_ids\n",
    "        )\n",
    "\n",
    "    # reward\n",
    "    chosen_rewards = chosen_logprobs - ref_chosen_logprobs\n",
    "    rejected_rewards = rejected_logprobs - ref_rejected_logprobs\n",
    "\n",
    "    logits = beta * (chosen_rewards - rejected_rewards)\n",
    "    loss = -F.logsigmoid(logits).mean()\n",
    "    accuracy = (logits > 0).float().mean()\n",
    "\n",
    "    return loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f79b5a",
   "metadata": {},
   "source": [
    "### Step 4：訓練迴圈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a238462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-7)\n",
    "\n",
    "scheduler = get_scheduler(\n",
    "    \"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=100,\n",
    "    num_training_steps=len(train_dataloader) * num_epochs\n",
    ")\n",
    "\n",
    "step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        loss, accuracy = compute_dpo_loss(model, ref_model, batch, beta=0.1)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        step += 1\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Loss: {loss.item():.4f}, Accuracy: {accuracy.item():.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5326d7bc",
   "metadata": {},
   "source": [
    "### 完整程式碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bfb085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_scheduler\n",
    "\n",
    "# =========================\n",
    "# Step 1: 資料準備\n",
    "# =========================\n",
    "preference_data = [\n",
    "    {\n",
    "        \"prompt\": \"解釋量子計算\",\n",
    "        \"chosen\": \"量子計算利用量子疊加和糾纏特性...\",\n",
    "        \"rejected\": \"量子計算就是很快的電腦\"\n",
    "    },\n",
    "    # 更多資料...\n",
    "]\n",
    "\n",
    "class PreferenceDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=256):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        prompt = item[\"prompt\"]\n",
    "        chosen = item[\"chosen\"]\n",
    "        rejected = item[\"rejected\"]\n",
    "\n",
    "        prompt_ids = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=self.max_len)\n",
    "        chosen_ids = self.tokenizer(chosen, return_tensors=\"pt\", truncation=True, max_length=self.max_len)\n",
    "        rejected_ids = self.tokenizer(rejected, return_tensors=\"pt\", truncation=True, max_length=self.max_len)\n",
    "\n",
    "        return {\n",
    "            \"prompt_ids\": prompt_ids[\"input_ids\"].squeeze(0),\n",
    "            \"prompt_mask\": prompt_ids[\"attention_mask\"].squeeze(0),\n",
    "            \"chosen_ids\": chosen_ids[\"input_ids\"].squeeze(0),\n",
    "            \"chosen_mask\": chosen_ids[\"attention_mask\"].squeeze(0),\n",
    "            \"rejected_ids\": rejected_ids[\"input_ids\"].squeeze(0),\n",
    "            \"rejected_mask\": rejected_ids[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # 將不同長度 pad 成同樣長度\n",
    "    def pad_tensor(tensors, pad_value=0):\n",
    "        max_len = max([t.size(0) for t in tensors])\n",
    "        padded = torch.full((len(tensors), max_len), pad_value, dtype=tensors[0].dtype)\n",
    "        for i, t in enumerate(tensors):\n",
    "            padded[i, :t.size(0)] = t\n",
    "        return padded\n",
    "\n",
    "    return {\n",
    "        \"prompt_ids\": pad_tensor([b[\"prompt_ids\"] for b in batch]),\n",
    "        \"prompt_mask\": pad_tensor([b[\"prompt_mask\"] for b in batch]),\n",
    "        \"chosen_ids\": pad_tensor([b[\"chosen_ids\"] for b in batch]),\n",
    "        \"chosen_mask\": pad_tensor([b[\"chosen_mask\"] for b in batch]),\n",
    "        \"rejected_ids\": pad_tensor([b[\"rejected_ids\"] for b in batch]),\n",
    "        \"rejected_mask\": pad_tensor([b[\"rejected_mask\"] for b in batch]),\n",
    "    }\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Step 2: 載入模型\n",
    "# =========================\n",
    "model = AutoModelForCausalLM.from_pretrained(\"your-sft-model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"your-sft-model\")\n",
    "\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(\"your-sft-model\")\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Step 3: 計算 DPO Loss\n",
    "# =========================\n",
    "def compute_logprobs(model, input_ids, attention_mask, labels):\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits  # [B, L, V]\n",
    "\n",
    "    shift_logits = logits[:, -labels.size(1):, :]\n",
    "    logprobs = F.log_softmax(shift_logits, dim=-1)\n",
    "\n",
    "    chosen_logprobs = torch.gather(\n",
    "        logprobs,\n",
    "        dim=-1,\n",
    "        index=labels.unsqueeze(-1)\n",
    "    ).squeeze(-1).sum(dim=-1)\n",
    "\n",
    "    return chosen_logprobs\n",
    "\n",
    "\n",
    "def compute_dpo_loss(model, ref_model, batch, beta=0.1):\n",
    "    prompt_ids = batch[\"prompt_ids\"]\n",
    "    prompt_mask = batch[\"prompt_mask\"]\n",
    "    chosen_ids = batch[\"chosen_ids\"]\n",
    "    chosen_mask = batch[\"chosen_mask\"]\n",
    "    rejected_ids = batch[\"rejected_ids\"]\n",
    "    rejected_mask = batch[\"rejected_mask\"]\n",
    "\n",
    "    chosen_input_ids = torch.cat([prompt_ids, chosen_ids], dim=1)\n",
    "    chosen_attn = torch.cat([prompt_mask, chosen_mask], dim=1)\n",
    "\n",
    "    rejected_input_ids = torch.cat([prompt_ids, rejected_ids], dim=1)\n",
    "    rejected_attn = torch.cat([prompt_mask, rejected_mask], dim=1)\n",
    "\n",
    "    chosen_logprobs = compute_logprobs(model, chosen_input_ids, chosen_attn, chosen_ids)\n",
    "    rejected_logprobs = compute_logprobs(model, rejected_input_ids, rejected_attn, rejected_ids)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ref_chosen_logprobs = compute_logprobs(ref_model, chosen_input_ids, chosen_attn, chosen_ids)\n",
    "        ref_rejected_logprobs = compute_logprobs(ref_model, rejected_input_ids, rejected_attn, rejected_ids)\n",
    "\n",
    "    chosen_rewards = chosen_logprobs - ref_chosen_logprobs\n",
    "    rejected_rewards = rejected_logprobs - ref_rejected_logprobs\n",
    "\n",
    "    logits = beta * (chosen_rewards - rejected_rewards)\n",
    "    loss = -F.logsigmoid(logits).mean()\n",
    "    accuracy = (logits > 0).float().mean()\n",
    "\n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Step 4: 訓練迴圈\n",
    "# =========================\n",
    "train_dataset = PreferenceDataset(preference_data, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, collate_fn=collate_fn)\n",
    "\n",
    "num_epochs = 3\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-7)\n",
    "\n",
    "scheduler = get_scheduler(\n",
    "    \"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=100,\n",
    "    num_training_steps=len(train_dataloader) * num_epochs\n",
    ")\n",
    "\n",
    "step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        loss, accuracy = compute_dpo_loss(model, ref_model, batch, beta=0.1)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        step += 1\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Loss: {loss.item():.4f}, Accuracy: {accuracy.item():.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ed5b22",
   "metadata": {},
   "source": [
    "### 使用 TRL 函式庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc314062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DPOTrainer, DPOConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. 載入模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\"your-sft-model\")\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(\"your-sft-model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"your-sft-model\")\n",
    "\n",
    "# 2. 準備資料集（必須包含這些欄位）\n",
    "dataset = load_dataset(\"your-preference-dataset\")\n",
    "# 資料格式：\n",
    "# {\n",
    "#     \"prompt\": \"...\",\n",
    "#     \"chosen\": \"...\",\n",
    "#     \"rejected\": \"...\"\n",
    "# }\n",
    "\n",
    "# 3. 設定訓練參數\n",
    "training_args = DPOConfig(\n",
    "    output_dir=\"./dpo_output\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=5e-7,\n",
    "    beta=0.1,  # DPO 的 β 參數\n",
    "    max_length=512,\n",
    "    max_prompt_length=128,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# 4. 初始化 Trainer\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=ref_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 5. 開始訓練\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714fd140",
   "metadata": {},
   "source": [
    "### DPO 的 β 參數調教\n",
    "\n",
    "β 是一個超參數，控制 **KL 散度懲罰的強度**：\n",
    "\n",
    "- β 大（如 0.5）：強制模型接近參考模型，變化保守\n",
    "- β 小（如 0.1）：允許模型更自由地優化偏好，但可能過度偏離\n",
    "\n",
    "**直覺理解：**\n",
    "\n",
    "β = 溫度的倒數 = \"我有多相信這個偏好資料\"\n",
    "\n",
    "- β 大 → 謹慎調整，不要改太多\n",
    "- β 小 → 大膽調整，充分利用偏好資訊\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08b11c8",
   "metadata": {},
   "source": [
    "## 實務注意事項\n",
    "\n",
    "### 資料品質至關重要\n",
    "\n",
    "**常見問題：**\n",
    "1. 偏好不明顯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03c4bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 壞範例：兩個回答都很好或都很差\n",
    "{\n",
    "    \"prompt\": \"1+1等於多少？\",\n",
    "    \"chosen\": \"2\",\n",
    "    \"rejected\": \"等於2\"  # 幾乎一樣，模型學不到什麼\n",
    "}\n",
    "\n",
    "# 好範例：清晰的品質差異\n",
    "{\n",
    "    \"prompt\": \"解釋相對論\",\n",
    "    \"chosen\": \"愛因斯坦的相對論包含狹義和廣義兩部分...\",\n",
    "    \"rejected\": \"就是時間會變慢\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17d96b5",
   "metadata": {},
   "source": [
    "2. 標註不一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b722d8a4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 確保標註者有明確的評分標準\n",
    "標準範例：\n",
    "- 有用性：回答是否解決問題？\n",
    "- 無害性：是否包含危險建議？\n",
    "- 誠實性：是否承認不確定性？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2838db98",
   "metadata": {},
   "source": [
    "3. 資料分布不均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f3a734",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 確保涵蓋各種場景\n",
    "distribution_check = {\n",
    "    \"coding\": 0.3,\n",
    "    \"math\": 0.2,\n",
    "    \"creative_writing\": 0.2,\n",
    "    \"qa\": 0.3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ba857c",
   "metadata": {},
   "source": [
    "### β 參數調教\n",
    "\n",
    "經驗法則："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53a3b44",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 小模型（<7B）\n",
    "beta = 0.1 ~ 0.2  # 較大的自由度\n",
    "\n",
    "# 中模型（7B-13B）\n",
    "beta = 0.1  # 標準選擇\n",
    "\n",
    "# 大模型（>13B）\n",
    "beta = 0.05 ~ 0.1  # 更保守\n",
    "\n",
    "# 領域適配（domain-specific）\n",
    "beta = 0.2 ~ 0.5  # 更接近原模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30547715",
   "metadata": {},
   "source": [
    "**診斷方法：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae6a76c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 監控 KL 散度\n",
    "kl_divergence = (chosen_logprobs - ref_chosen_logprobs).mean()\n",
    "\n",
    "if kl_divergence > 10:\n",
    "    print(\"β 太小，模型偏離過大\")\n",
    "elif kl_divergence < 0.1:\n",
    "    print(\"β 太大，模型幾乎沒變化\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4a73f0",
   "metadata": {},
   "source": [
    "### 學習率設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b01afa7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# DPO 需要比 SFT 更小的學習率\n",
    "# 推薦範圍\n",
    "learning_rate = 5e-7  # SFT 通常是 2e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73662603",
   "metadata": {},
   "source": [
    "### 監控偏好準確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3776c9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 監控的指標\n",
    "metrics = {\n",
    "    \"loss\": loss.item(),\n",
    "    \"accuracy\": (chosen_rewards > rejected_rewards).float().mean(),\n",
    "    \"chosen_reward\": chosen_rewards.mean(),\n",
    "    \"rejected_reward\": rejected_rewards.mean(),\n",
    "    \"reward_margin\": (chosen_rewards - rejected_rewards).mean(),\n",
    "    \"kl_div\": kl_divergence.mean()\n",
    "}\n",
    "\n",
    "# 健康的訓練應該看到：\n",
    "# - accuracy 從 ~50% 提升到 >70%\n",
    "# - reward_margin 逐漸增大\n",
    "# - kl_div 保持在合理範圍（<10）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570e51c5",
   "metadata": {},
   "source": [
    "## 常見踩雷點\n",
    "### 忘記凍結 ref_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16300c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 錯誤：ref_model 也在更新\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(\"model\")\n",
    "\n",
    "# 正確：凍結參數\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(\"model\")\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False\n",
    "ref_model.eval()  # 切換到評估模式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b009c1",
   "metadata": {},
   "source": [
    "### chosen/rejected 長度差異過大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1cdcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 問題：rejected 太短，loss 不平衡\n",
    "{\n",
    "    \"chosen\": \"這是一段很長的詳細解釋...\",  # 200 tokens\n",
    "    \"rejected\": \"不知道\"  # 2 tokens\n",
    "}\n",
    "\n",
    "# 解決：控制長度比例\n",
    "max_length_ratio = len(rejected) / len(chosen)\n",
    "if max_length_ratio < 0.3:\n",
    "    print(\"警告：長度差異過大，考慮重新採樣\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3ce7b7",
   "metadata": {},
   "source": [
    "## DPO 的變體\n",
    "近期研究提出了多個 DPO 改進版本：\n",
    "1. IPO (Identity Preference Optimization)\n",
    "    - 移除了 sigmoid，直接改善 margin\n",
    "    - 對長度偏差更穩定\n",
    "2. KTO (Kahneman-Tversky Optimization)\n",
    "    - 不需要配對資料，只需要 binary feedback\n",
    "    - 適合「讚/踩」這種簡單回饋\n",
    "3. SimPO (Simple Preference Optimization)\n",
    "    - 不需要 reference model\n",
    "    - 直接用 length normalization"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
